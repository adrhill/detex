{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"asdex","text":"<p>Automatic Sparse Differentiation in JAX.</p> <p><code>asdex</code> (pronounced Aztecs) exploits sparsity structure to efficiently compute sparse Jacobians and Hessians. It implements a custom Jaxpr interpreter that uses abstract interpretation to detect global sparsity patterns from the computation graph, then uses graph coloring to minimize the number of AD passes needed. Refer to our Illustrated Guide to Automatic Sparse Differentiation for more information.</p> <p>Alpha Software</p> <p><code>asdex</code> is in early development. The API may change without notice. Use at your own risk.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/adrhill/asdex.git\n</code></pre> <p>Or with uv:</p> <pre><code>uv add git+https://github.com/adrhill/asdex.git\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import numpy as np\nfrom asdex import jacobian\n\ndef f(x):\n    return (x[1:] - x[:-1]) ** 2\n\nx = np.random.randn(1000)\n\njac_fn = jacobian(f, input_shape=x.shape)\nJ = jac_fn(x)\n</code></pre> <p>Instead of 999 VJPs or 1000 JVPs, <code>asdex</code> computes the full sparse Jacobian with just 2 JVPs.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started \u2014 step-by-step tutorial</li> <li>How-To Guides \u2014 task-oriented recipes</li> <li>Explanation \u2014 how and why it works</li> <li>API Reference \u2014 full API documentation</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This package is built with Claude Code based on previous work by Adrian Hill, Guillaume Dalle, and Alexis Montoison in the Julia programming language:</p> <ul> <li>An Illustrated Guide to Automatic Sparse Differentiation, A. Hill, G. Dalle, A. Montoison (2025)</li> <li>Sparser, Better, Faster, Stronger: Efficient Automatic Differentiation for Sparse Jacobians and Hessians, A. Hill &amp; G. Dalle (2025)</li> <li>Revisiting Sparse Matrix Coloring and Bicoloring, A. Montoison, G. Dalle, A. Gebremedhin (2025)</li> <li>SparseConnectivityTracer.jl, A. Hill, G. Dalle</li> <li>SparseMatrixColorings.jl, G. Dalle, A. Montoison</li> <li>sparsediffax, G. Dalle</li> </ul> <p>which in turn stands on the shoulders of giants \u2014 notably Andreas Griewank, Andrea Walther, and Assefaw Gebremedhin.</p>"},{"location":"explanation/asd/","title":"Automatic Sparse Differentiation","text":"<p>Automatic sparse differentiation (ASD) exploits the sparsity structure of Jacobians and Hessians to compute them far more efficiently than dense automatic differentiation (AD). This page explains the mathematical ideas behind the approach.</p>"},{"location":"explanation/asd/#automatic-differentiation-in-brief","title":"Automatic Differentiation in Brief","text":"<p>AD computes exact derivatives of programs by applying the chain rule to each elementary operation. For a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), AD can evaluate Jacobian-vector products in two modes:</p> <ul> <li>Reverse mode computes a vector-Jacobian product (VJP): given a covector \\(v \\in \\mathbb{R}^m\\),   it returns \\(v^\\top J\\) in roughly the same time as one evaluation of \\(f\\).   Setting \\(v = e_i\\) extracts row \\(i\\) of the Jacobian.</li> <li>Forward mode computes a Jacobian-vector product (JVP): given a tangent vector \\(t \\in \\mathbb{R}^n\\),   it returns \\(Jt\\) in roughly the same time as one evaluation of \\(f\\).   Setting \\(t = e_j\\) extracts column \\(j\\) of the Jacobian.</li> </ul> <p>In both cases, one AD pass produces a single row or column \u2014 not the full Jacobian. Building the complete matrix requires multiple passes, and reducing their number is the goal of sparse differentiation.</p>"},{"location":"explanation/asd/#computational-cost-of-ad","title":"Computational Cost of AD","text":"<p>Consider a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) with Jacobian \\(J \\in \\mathbb{R}^{m \\times n}\\). Computing \\(J\\) by standard autodiff requires either:</p> <ul> <li>\\(m\\) VJPs (reverse-mode): one per row, or</li> <li>\\(n\\) JVPs (forward-mode): one per column.</li> </ul> <p>For large, sparse Jacobians \u2014 common in scientific computing \u2014 most entries of \\(J\\) are zero, yet dense autodiff pays the full cost of \\(m\\) or \\(n\\) passes. ASD reduces this to a number of passes proportional to the number of colors in a graph coloring of the sparsity pattern, which is often orders of magnitude smaller.</p>"},{"location":"explanation/asd/#structural-orthogonality","title":"Structural Orthogonality","text":"<p>The key insight is structural orthogonality: two rows \\(i_1\\) and \\(i_2\\) of \\(J\\) are structurally orthogonal when they share no nonzero column \u2014 that is, there is no column \\(j\\) where both \\(J_{i_1 j}\\) and \\(J_{i_2 j}\\) are (potentially) nonzero.</p> <p>If two rows are structurally orthogonal, their VJPs can be combined into a single pass by summing their seed vectors. The nonzeros don't overlap, so the result is unambiguous. The same idea applies symmetrically to columns and JVPs.</p>"},{"location":"explanation/asd/#seed-matrices-and-compression","title":"Seed Matrices and Compression","text":"<p>Graph coloring assigns each row a color such that rows sharing a nonzero column get different colors. From this coloring, we build a seed matrix \\(S \\in \\mathbb{R}^{m \\times c}\\), where \\(c\\) is the number of colors. The seed for color \\(c\\) is the sum of the standard basis vectors for all rows assigned that color:</p> \\[ S_{:,c} = \\sum_{i \\,:\\, \\text{color}(i) = c} e_i \\] <p>The compressed Jacobian is then:</p> \\[ B = S^\\top \\cdot J \\in \\mathbb{R}^{c \\times n} \\] <p>Computing \\(B\\) requires only \\(c\\) VJPs \u2014 one per color \u2014 instead of \\(m\\).</p>"},{"location":"explanation/asd/#decompression","title":"Decompression","text":"<p>Recovering the sparse Jacobian from \\(B\\) is called decompression. Because same-colored rows are structurally orthogonal, each nonzero \\(J_{ij}\\) appears in exactly one entry of \\(B\\):</p> \\[ J_{ij} = B_{\\text{color}(i),\\, j} \\] <p>We simply read off each nonzero from the compressed matrix using the known color assignments. This is direct decompression \u2014 no systems of equations need to be solved.</p>"},{"location":"explanation/asd/#the-three-step-pipeline","title":"The Three-Step Pipeline","text":"<p>ASD decomposes the problem into three independent stages:</p> <ol> <li>Detection \u2014 determine the global sparsity pattern of the Jacobian    by analyzing the computation graph (no numerical evaluation).</li> <li>Coloring \u2014 assign colors to rows or columns    so that structurally orthogonal groups share a color.</li> <li>Decompression \u2014 compute one AD pass per color and extract the sparse matrix.</li> </ol> <p>Steps 1 and 2 are preprocessing: they depend only on the function's structure and input shape, not on the input values. Once computed, the coloring can be reused across arbitrarily many evaluations. Step 3 is the only part that touches actual numerical data.</p>"},{"location":"explanation/asd/#amortization","title":"Amortization","text":"<p>The three-step split is designed around amortization. Detection and coloring are the most expensive steps, but their results depend only on the function's structure and input shape \u2014 not on the input values. This means they can be computed once and reused across arbitrarily many evaluations at different inputs.</p> <p>In a typical workflow, a user calls <code>jacobian_coloring</code> (or <code>hessian_coloring</code>) once during setup and passes the result to <code>jacobian</code> (or <code>hessian</code>) in a loop. The per-evaluation cost is then just the decompression step: \\(c\\) AD passes plus a cheap index lookup, where \\(c\\) is the number of colors. For problems where the Jacobian is evaluated hundreds or thousands of times \u2014 such as implicit solvers, optimization, or time-stepping \u2014 the preprocessing cost becomes negligible.</p> <p>This amortization assumption also guides design decisions in <code>asdex</code>: it is worth spending more time on detection if it produces sparser patterns, because fewer nonzeros lead to fewer colors and fewer AD passes on every subsequent evaluation.</p>"},{"location":"explanation/asd/#extension-to-hessians","title":"Extension to Hessians","text":"<p>For a scalar function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\), the Hessian \\(H \\in \\mathbb{R}^{n \\times n}\\) is symmetric: \\(H_{ij} = H_{ji}\\).</p> <p>Since the Hessian is the Jacobian of the gradient, sparsity detection reduces to Jacobian detection: \\(\\operatorname{hessian\\_sparsity}(f) = \\operatorname{jacobian\\_sparsity}(\\nabla f)\\). This works because <code>jax.grad</code> produces a jaxpr like any other function, so the same interpreter handles both cases with no extra machinery. And because the Hessian is symmetric, coloring can exploit this via star coloring, which typically needs far fewer colors than treating the Hessian as a general matrix.</p> <p>Each color corresponds to one Hessian-vector product (HVP), computed via forward-over-reverse autodiff. The decompression step recovers both \\(H_{ij}\\) and \\(H_{ji}\\) from each entry, halving the effective number of unknowns.</p>"},{"location":"explanation/asd/#references","title":"References","text":"<ul> <li>An Illustrated Guide to Automatic Sparse Differentiation, Hill, Dalle, Montoison (2025) \u2014 a visual walkthrough of the ideas on this page.</li> </ul>"},{"location":"explanation/coloring/","title":"Graph Coloring","text":"<p>Graph coloring is the key technique that makes automatic sparse differentiation efficient. This page explains how the conflict graph is built, the coloring variants <code>asdex</code> supports, and the algorithm used to find colorings.</p>"},{"location":"explanation/coloring/#the-conflict-graph","title":"The Conflict Graph","text":"<p>Given a sparsity pattern, we build a conflict graph whose vertices are the rows (or columns) of the matrix and whose edges connect pairs that share a nonzero column (or row). A proper coloring of this graph assigns colors to vertices so that no two adjacent vertices share a color. Vertices with the same color are then guaranteed to be structurally orthogonal, meaning they can share an AD pass. The total number of passes equals the number of colors, which is often dramatically fewer than the matrix dimension.</p>"},{"location":"explanation/coloring/#row-and-column-coloring","title":"Row and Column Coloring","text":"<p>There are two variants. Row coloring treats rows as vertices and connects rows that share a nonzero column; same-colored rows are evaluated together using VJPs (reverse-mode AD). Column coloring treats columns as vertices and connects columns that share a nonzero row; same-colored columns are evaluated together using JVPs (forward-mode AD). By default, <code>asdex</code> tries both and picks whichever needs fewer colors. When tied, it prefers column coloring since JVPs are generally cheaper in JAX.</p>"},{"location":"explanation/coloring/#symmetric-coloring-for-hessians","title":"Symmetric Coloring for Hessians","text":"<p>Hessians are symmetric (\\(H_{ij} = H_{ji}\\)), so each off-diagonal entry appears twice in the matrix. Exploiting this redundancy can significantly reduce the number of colors needed, since recovering \\(H_{ij}\\) from a compressed column simultaneously gives us \\(H_{ji}\\) for free. The coloring operates on an adjacency graph whose vertices are variables and whose edges connect pairs \\(i, j\\) with \\(H_{ij} \\neq 0\\). Diagonal entries are always recoverable, so only off-diagonal nonzeros create edges.</p> <p><code>asdex</code> uses star coloring (Gebremedhin et al., 2005) on this graph: a proper coloring with the additional constraint that every path on 4 vertices uses at least 3 colors. This constraint ensures that for each off-diagonal nonzero \\(H_{ij}\\), at least one of \\(i\\) or \\(j\\) has a unique color among the other's neighbors, making every entry unambiguously recoverable from the compressed product. Star coloring typically needs far fewer colors than treating the Hessian as a general Jacobian and applying row or column coloring.</p>"},{"location":"explanation/coloring/#the-greedy-algorithm","title":"The Greedy Algorithm","text":"<p><code>asdex</code> colors graphs using a greedy algorithm with LargestFirst vertex ordering. Vertices are sorted by decreasing degree (number of conflicts), and each vertex is assigned the smallest color not already used by any of its neighbors. Handling high-degree vertices first tends to produce fewer colors in practice, because the most constrained vertices are colored while the most options are still available.</p> <p>The greedy algorithm does not guarantee an optimal coloring, but it is fast and produces good results for the sparsity patterns typically encountered in scientific computing.</p>"},{"location":"explanation/coloring/#from-coloring-to-decompression","title":"From Coloring to Decompression","text":"<p>The coloring result is materialized as a seed matrix whose columns are indicator vectors \u2014 one per color group (see Seed Matrices and Compression). Multiplying the seed matrix by the Jacobian (or evaluating the corresponding JVPs/VJPs) produces a compressed matrix with one row or column per color.</p> <p>Recovering the sparse matrix from the compressed product is called decompression. Because same-colored rows or columns are structurally orthogonal, each nonzero appears in exactly one position of the compressed matrix and can be read off directly \u2014 no linear systems need to be solved.</p> <p>For symmetric Hessians the story is similar, but each off-diagonal entry \\(H_{ij}\\) can be recovered from either the row of \\(i\\) or the row of \\(j\\) in the compressed product. The star coloring guarantee ensures that at least one direction is always unambiguous.</p> <p>See the API reference for <code>jacobian_coloring</code> and <code>hessian_coloring</code>.</p>"},{"location":"explanation/coloring/#references","title":"References","text":"<ul> <li>Revisiting Sparse Matrix Coloring and Bicoloring, Montoison et al. (2025)</li> <li>What Color Is Your Jacobian? Graph Coloring for Computing Derivatives, Gebremedhin et al. (2005)</li> <li>New Acyclic and Star Coloring Algorithms with Application to Computing Hessians, Gebremedhin et al. (2007)</li> <li>Efficient Computation of Sparse Hessians Using Coloring and Automatic Differentiation, Gebremedhin et al. (2009)</li> <li>ColPack: Software for graph coloring and related problems in scientific computing, Gebremedhin et al. (2013)</li> <li>SparseMatrixColorings.jl, Dalle &amp; Montoison (2025) \u2014 the Julia library from which the <code>asdex</code> coloring implementation is adapted</li> </ul>"},{"location":"explanation/global-sparsity/","title":"Global Sparsity Patterns","text":"<p>Before detecting sparsity or coloring a matrix, we need to decide which sparsity pattern to work with. This page explains the distinction between local and global patterns and why <code>asdex</code> uses global patterns exclusively.</p>"},{"location":"explanation/global-sparsity/#local-vs-global-patterns","title":"Local vs. Global Patterns","text":"<p>Consider \\(f(x) = x_1 \\cdot x_2\\), whose Jacobian is \\([x_2,\\; x_1]\\).</p> <p>A local pattern is the sparsity at a specific input point \\(x\\), so it depends on the numerical values. At \\(x = (1, 0)\\), the local pattern is <code>[0, 1]</code>, but at \\(x = (0, 1)\\) it is <code>[1, 0]</code>.</p> <p>A global pattern is the union of local patterns over the entire input domain, making it input-independent. For the same example, the global pattern is always <code>[1, 1]</code> \u2014 no sparsity. Global patterns are supersets of local patterns: less sparse, but valid everywhere.</p>"},{"location":"explanation/global-sparsity/#why-conservative-patterns-are-safe","title":"Why Conservative Patterns Are Safe","text":"<p>A sparsity pattern used for coloring must be either accurate or conservative.</p> <p>If the pattern misses a nonzero entry (under-approximation), the coloring may merge rows or columns that actually conflict, silently producing wrong results. There is no way to detect this error after the fact.</p> <p>If the pattern includes extra nonzeros (over-approximation), the coloring simply uses more colors than strictly necessary. The computed Jacobian is still correct \u2014 just slightly less efficient to obtain.</p> <p>This asymmetry is why <code>asdex</code> errs on the side of conservatism: correctness comes first.</p>"},{"location":"explanation/global-sparsity/#why-global-over-local","title":"Why Global Over Local","text":"<p>Sparsity detection and graph coloring are preprocessing steps that happen before any Jacobian is actually computed. Both are expensive enough that we want their results to be reusable across many evaluations at different input points.</p> <p>A global pattern is input-independent by construction, so the detection and coloring work is done once and amortized. A local pattern would need to be recomputed every time the input changes, negating most of the efficiency gains.</p> <p><code>asdex</code> achieves input-independence by using abstract interpretation rather than numerical evaluation. Instead of plugging in concrete numbers, it propagates index sets through the computation graph. The result depends only on the function's structure, not on any particular input. The details are covered in Sparsity Detection.</p>"},{"location":"explanation/global-sparsity/#the-trade-off","title":"The Trade-Off","text":"<p>Global patterns may be less sparse than local ones. In the \\(f(x) = x_1 \\cdot x_2\\) example, the global pattern is fully dense even though every local pattern has a zero. This means a few extra colors and a few extra AD passes.</p> <p>In practice, the trade-off is overwhelmingly favorable: the cost of extra colors is small, while the ability to reuse a single coloring across all inputs is what makes automatic sparse differentiation practical.</p> <p>Tip</p> <p>If a sparsity pattern looks overly conservative for your function, please help out <code>asdex</code>'s development by reporting it. These reports directly drive improvements and are one of the most impactful ways to contribute.</p>"},{"location":"explanation/global-sparsity/#precision-over-speed","title":"Precision Over Speed","text":"<p><code>asdex</code> is designed around the assumption that detection and coloring costs are amortized over many Jacobian evaluations. Given this, it favors sparser patterns even if detecting them is slower: fewer nonzeros lead to fewer colors, and fewer colors mean fewer AD passes on every subsequent evaluation. The one-time cost of a more precise analysis is quickly repaid by cheaper evaluations.</p>"},{"location":"explanation/sparsity-detection/","title":"Sparsity Detection","text":"<p>Sparsity detection determines which entries of a Jacobian can be nonzero. Given \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), it computes a binary global sparsity pattern \u2014 a conservative superset of the true nonzero structure of the Jacobian \\(J_{ij} = \\partial f_i / \\partial x_j\\). This pattern is the input to graph coloring, which exploits the sparsity to reduce the number of AD passes.</p>"},{"location":"explanation/sparsity-detection/#why-abstract-interpretation","title":"Why Abstract Interpretation","text":"<p>Since we need global sparsity patterns \u2014 patterns that are valid for all inputs, not just a specific one \u2014 numerical approaches like finite-difference probing or computing the Jacobian via AD are not suitable. They evaluate the function at a concrete input and can only reveal the local pattern at that point, missing nonzeros that happen to be zero at the probe.</p> <p><code>asdex</code> uses abstract interpretation instead: it analyzes the structure of the computation without evaluating it on real numbers. This gives global patterns directly \u2014 no numerical evaluation, no dependence on a particular input, and no risk of missing nonzeros due to cancellation.</p>"},{"location":"explanation/sparsity-detection/#jaxpr-jaxs-intermediate-representation","title":"Jaxpr: JAX's Intermediate Representation","text":"<p>JAX represents computations as jaxprs (JAX expressions) \u2014 a flat sequence of primitive operations with explicit data flow. When you call <code>jax.make_jaxpr(f)(x)</code>, JAX traces <code>f</code> symbolically (without evaluating it) and returns a jaxpr that records every operation.</p> <p>A jaxpr consists of:</p> <ul> <li>Input variables \u2014 the function's arguments.</li> <li>Equations \u2014 one per primitive operation (e.g. <code>sin</code>, <code>add</code>, <code>gather</code>),   each with input atoms, output variables, and parameters.</li> <li>Output variables \u2014 the function's return values.</li> </ul> <p>For example, <code>f(x) = sin(x) + 1</code> produces the following jaxpr:</p> <pre><code>{ lambda ; a:f32[3]. let\n    b:f32[3] = sin a\n    c:f32[3] = add b 1.0\n  in (c,) }\n</code></pre> <p><code>asdex</code> hooks into this representation: it calls <code>jax.make_jaxpr</code> to obtain the computation graph, then walks the equations one by one, propagating index sets instead of numerical values.</p>"},{"location":"explanation/sparsity-detection/#index-set-propagation","title":"Index Set Propagation","text":"<p>The core idea is to track, for each element of each intermediate array, which input elements it depends on. This dependency information is stored as index sets \u2014 a list of <code>set[int]</code>, one set per array element.</p> <p>The algorithm proceeds in three steps:</p> <p>1. Initialization. Each input element \\(x_j\\) starts with the singleton set \\(\\{j\\}\\), meaning it depends only on itself. For a 3-element input, the initial index sets are <code>[{0}, {1}, {2}]</code>.</p> <p>2. Propagation. Walk through each equation in the jaxpr. For each primitive, a handler maps input index sets to output index sets according to how that operation routes dependencies.</p> <p>3. Extraction. After processing all equations, read the index sets of the output variables. Output \\(i\\) depends on input \\(j\\) iff \\(j \\in S_i\\), which directly gives the sparsity pattern.</p>"},{"location":"explanation/sparsity-detection/#example","title":"Example","text":"<p>Consider \\(f(x) = \\bigl[\\sin(x_1 \\cdot x_2),\\; x_2 + x_3\\bigr]\\) with input \\(x \\in \\mathbb{R}^3\\).</p> <p>Initialization:</p> Variable Index sets \\(x\\) <code>[{0}, {1}, {2}]</code> <p>Equation 1: <code>mul x[0] x[1] \u2192 t1</code> \u2014 elementwise multiply. Each output depends on both operands:</p> Variable Index sets \\(t_1\\) <code>[{0, 1}]</code> <p>Equation 2: <code>sin t1 \u2192 t2</code> \u2014 elementwise unary op. Dependencies pass through unchanged:</p> Variable Index sets \\(t_2\\) <code>[{0, 1}]</code> <p>Equation 3: <code>add x[1] x[2] \u2192 t3</code> \u2014 elementwise add. Union of both operands:</p> Variable Index sets \\(t_3\\) <code>[{1, 2}]</code> <p>Extraction: The output is \\([t_2, t_3]\\), so the index sets are <code>[{0, 1}, {1, 2}]</code>. This encodes the sparsity pattern:</p> \\[ J = \\begin{pmatrix} \\times &amp; \\times &amp; \\\\ &amp; \\times &amp; \\times \\end{pmatrix} \\] <p>The true Jacobian is \\(\\bigl[\\begin{smallmatrix} x_2 \\cos(x_1 x_2) &amp; x_1 \\cos(x_1 x_2) &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\end{smallmatrix}\\bigr]\\), which confirms the detected pattern.</p>"},{"location":"explanation/sparsity-detection/#primitive-handlers","title":"Primitive Handlers","text":"<p>Each JAX primitive has a handler that defines how it propagates index sets. The handlers fall into a few families.</p>"},{"location":"explanation/sparsity-detection/#elementwise-operations","title":"Elementwise Operations","text":"<p>Unary operations like <code>sin</code>, <code>exp</code>, <code>neg</code> pass each element's index set through unchanged \u2014 \\(y_i = g(x_i)\\) means \\(S_{y_i} = S_{x_i}\\).</p> <p>Binary operations like <code>add</code>, <code>mul</code>, <code>sub</code> take the union of their operands' index sets \u2014 \\(y_i = x_i \\oplus z_i\\) means \\(S_{y_i} = S_{x_i} \\cup S_{z_i}\\). When shapes differ, broadcasting rules determine which elements pair up.</p>"},{"location":"explanation/sparsity-detection/#reductions","title":"Reductions","text":"<p>A reduction like <code>sum</code> over an axis unions all index sets along that axis. If \\(y_i = \\sum_k x_{ik}\\), then \\(S_{y_i} = \\bigcup_k S_{x_{ik}}\\). A full reduction (no remaining axes) unions everything into a single set.</p>"},{"location":"explanation/sparsity-detection/#permutations-and-reshapes","title":"Permutations and Reshapes","text":"<p>Operations like <code>transpose</code>, <code>reshape</code>, <code>slice</code>, <code>reverse</code>, and <code>broadcast</code> rearrange elements without combining them. Each output element maps to exactly one input element, so the handler copies the corresponding index set. <code>asdex</code> implements this via a position map: apply the operation to an array where each element holds its own flat index, then read off the mapping.</p>"},{"location":"explanation/sparsity-detection/#indexing-gather-and-scatter","title":"Indexing (Gather and Scatter)","text":"<p><code>gather</code> and <code>scatter</code> are the most complex primitives. When the indices are statically known (constants in the jaxpr), the handler resolves exactly which input position each output reads from and copies index sets accordingly \u2014 just like a permutation.</p> <p>When the indices are dynamic (computed from inputs), the handler cannot know which elements will be accessed at runtime. It falls back to the conservative strategy described below.</p> <p>This is why <code>asdex</code> tracks constant values through the computation graph: if an index array is built from literals and arithmetic on constants, the handler can still resolve it precisely even though it is not a direct literal in the jaxpr.</p>"},{"location":"explanation/sparsity-detection/#fallback-handlers","title":"Fallback Handlers","text":"<p>Not every JAX primitive has a precise handler. When <code>asdex</code> encounters an unhandled primitive, it uses a conservative fallback: every output element is assumed to depend on every input element. This is always correct \u2014 it is a superset of the true pattern \u2014 but it may be much less sparse than necessary.</p> <p>A small number of primitives use this fallback intentionally (e.g. callbacks into opaque Python code where dependencies cannot be analyzed). For all other cases, <code>asdex</code> raises an error on unknown primitives rather than silently falling back, since silent over-approximation could mask bugs in the handler coverage.</p> <p>Tip</p> <p>More precise handlers can be added for fallback primitives to reduce conservatism and produce sparser patterns. Please open an issue if you encounter overly conservative patterns.</p>"},{"location":"explanation/sparsity-detection/#sources-of-conservatism","title":"Sources of Conservatism","text":"<p>Even with precise handlers, three mechanisms make global patterns conservative relative to local ones:</p> <ol> <li>Branching (<code>cond</code>, <code>select_n</code>):    the detector takes the union over all branches,    since it cannot know which branch will execute at runtime.    This is the primary difference from local detection.</li> <li>Multiplication:    \\(f(x) = x_1 \\cdot x_2\\) always reports both dependencies globally,    even though one factor might be zero at a particular input.</li> <li>Dynamic indexing:    when gather/scatter indices depend on the input,    the handler must assume any element could be accessed.</li> </ol>"},{"location":"explanation/sparsity-detection/#hessian-detection","title":"Hessian Detection","text":"<p>Hessian sparsity is detected by applying Jacobian detection to the gradient:</p> \\[ \\operatorname{hessian\\_sparsity}(f) = \\operatorname{jacobian\\_sparsity}(\\nabla f) \\] <p>This composes naturally with JAX's autodiff: <code>jax.grad</code> produces a jaxpr, which <code>asdex</code> analyzes the same way.</p>"},{"location":"how-to/brusselator/","title":"Example: Brusselator PDE","text":"<p>This example computes the sparse Jacobian of a semi-discretized 2D reaction-diffusion system using <code>asdex</code>.</p>"},{"location":"how-to/brusselator/#the-brusselator-system","title":"The Brusselator System","text":"<p>The Brusselator models autocatalytic reactions on a 2D domain \\([0, 1]^2\\) with periodic boundary conditions:</p> \\[ \\frac{\\partial u}{\\partial t} = 1 + u^2 v - 4.4\\,u + \\alpha \\nabla^2 u \\] \\[ \\frac{\\partial v}{\\partial t} = 3.4\\,u - u^2 v + \\alpha \\nabla^2 v \\] <p>with diffusion coefficient \\(\\alpha = 10\\). See the SciML Brusselator tutorials for the full formulation including a localized forcing term, which is omitted here since it is state-independent and does not affect the Jacobian sparsity.</p>"},{"location":"how-to/brusselator/#discretizing-the-rhs","title":"Discretizing the RHS","text":"<p>Semi-discretize in space using second-order finite differences on an \\(N \\times N\\) grid. The state vector concatenates both species \\(u\\) and \\(v\\), giving \\(2N^2\\) unknowns:</p> <pre><code>import jax.numpy as jnp\n\nN = 32\nalpha = 10.0\ndx = 1.0 / N\n\ndef brusselator_rhs(uv):\n    u = uv[:N*N].reshape(N, N)\n    v = uv[N*N:].reshape(N, N)\n\n    # 5-point Laplacian with periodic boundary conditions\n    def laplacian(w):\n        return (\n            jnp.roll(w, 1, axis=0) + jnp.roll(w, -1, axis=0)\n            + jnp.roll(w, 1, axis=1) + jnp.roll(w, -1, axis=1)\n            - 4 * w\n        ) / dx**2\n\n    du = 1.0 + u**2 * v - 4.4 * u + alpha * laplacian(u)\n    dv = 3.4 * u - u**2 * v + alpha * laplacian(v)\n\n    return jnp.concatenate([du.ravel(), dv.ravel()])\n</code></pre>"},{"location":"how-to/brusselator/#detecting-and-coloring-the-jacobian","title":"Detecting and Coloring the Jacobian","text":"<p>The Jacobian has shape \\(2048 \\times 2048\\), but only 6 nonzeros per row (5 from the Laplacian stencil plus 1 from reaction coupling). Detect the sparsity and color it in one call:</p> <pre><code>from asdex import jacobian_coloring\n\ncoloring = jacobian_coloring(brusselator_rhs, input_shape=2 * N * N)\n</code></pre> <pre><code>ColoredPattern(2048\u00d72048, nnz=12288, sparsity=99.7%, JVP, 12 colors)\n  12 JVPs (instead of 2048 VJPs or 2048 JVPs)\n\u23a1\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u28ff\u28ff\u28ff\u28ff\u28c3\u2840\u23a4\n\u23a2\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28cd\u2803\u23a5\n\u23a2\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28df\u28b7\u2801\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ef\u28db\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u286b\u2815\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ee\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2804\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28df\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28fd\u2800\u23a5\n\u23a2\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ee\u28c1\u23a5\n\u23a2\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u23a5 \u2192 \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u284f\u23a5\n\u23a2\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28df\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2803\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u28c4\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2877\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2844\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u23a6   \u23a3\u28ff\u28ff\u28ff\u28ff\u28ff\u28f6\u23a6\n</code></pre> <p>Instead of 2048 JVPs or VJPs, <code>asdex</code> needs only as many as there are colors.</p>"},{"location":"how-to/brusselator/#computing-the-jacobian","title":"Computing the Jacobian","text":"<p>With the coloring precomputed, evaluate the sparse Jacobian at any state:</p> <pre><code>from asdex import jacobian_from_coloring\n\n# Brusselator initial condition\nx = jnp.linspace(0, 1, N, endpoint=False)\nxx, yy = jnp.meshgrid(x, x)\nu0 = 22.0 * (yy * (1 - yy)) ** 1.5\nv0 = 27.0 * (xx * (1 - xx)) ** 1.5\nuv0 = jnp.concatenate([u0.ravel(), v0.ravel()])\n\njac_fn = jacobian_from_coloring(brusselator_rhs, coloring)\nJ = jac_fn(uv0)\n</code></pre> <pre><code>BCOO(float32[2048, 2048], nse=12288)\n</code></pre> <p>Make sure to reuse the <code>coloring</code> across evaluations at different states, such that only the decompression step is repeated.</p>"},{"location":"how-to/brusselator/#references","title":"References","text":"<p>This example is based on tutorials from the SciML ecosystem. Consider giving the Julia programming language a shot, it is fantastic.</p> <ul> <li>NonlinearSolve.jl: Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia   (MIT License, Copyright (c) 2020 Julia Computing, Inc.)</li> <li>MethodOfLines.jl: Getting Started    (MIT License, Copyright (c) 2022 SciML Open Source Scientific Machine Learning Organization.)</li> </ul>"},{"location":"how-to/hessians/","title":"Computing Sparse Hessians","text":"<p><code>asdex</code> computes sparse Hessians for scalar-valued functions \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) using symmetric coloring and forward-over-reverse AD.</p> <p>Verify correctness at least once</p> <p>asdex's sparsity patterns should always be conservative, but a bug in sparsity detection could cause missing nonzeros. Always verify against vanilla JAX at least once on a new function. See Verifying Results below.</p>"},{"location":"how-to/hessians/#basic-usage","title":"Basic Usage","text":"<p>Pass your scalar-valued function and its <code>input_shape</code> to <code>hessian</code>:</p> <pre><code>from asdex import hessian\n\nhess_fn = hessian(f, input_shape=100)\nH = hess_fn(x)\n</code></pre> <p>This runs the computationally expensive sparsity detection and coloring steps when defining <code>hess_fn</code>. Subsequent calls to <code>hess_fn</code> only need to perform the cheap decompression step. The result is a JAX BCOO sparse matrix.</p> <p>The same function can be reused across evaluations at different inputs:</p> <pre><code>for x in inputs:\n    H = hess_fn(x)\n</code></pre> <p><code>asdex</code> supports multi-dimensional input arrays. The Hessian is always returned as a 2D matrix of shape \\((n, n)\\) where \\(n\\) is the total number of input elements.</p>"},{"location":"how-to/hessians/#precomputing-the-colored-pattern","title":"Precomputing the Colored Pattern","text":"<p>For more control, precompute the coloring explicitly:</p> <pre><code>import jax.numpy as jnp\nfrom asdex import hessian_coloring, hessian_from_coloring\n\ndef g(x):\n    return jnp.sum((1 - x[:-1]) ** 2 + 100 * (x[1:] - x[:-1] ** 2) ** 2)\n\ncoloring = hessian_coloring(g, input_shape=100)\n</code></pre> <pre><code>ColoredPattern(100\u00d7100, nnz=298, sparsity=97.0%, HVP, 3 colors)\n  3 HVPs (instead of 100 HVPs)\n\u23a1\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u2877\u2847\u23a4\n\u23a2\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2845\u23a5\n\u23a2\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ef\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2845\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ef\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2845\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28df\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u23a5   \u23a2\u28df\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u23a6   \u23a3\u28ff\u2803\u23a6\n</code></pre> <p>This is useful when you want to visually inspect the coloring for correctness, or save it to disk to avoid recomputation. Pass the coloring to <code>hessian_from_coloring</code> to compute the Hessian:</p> <pre><code>hess_fn = hessian_from_coloring(g, coloring)\n\nfor x in inputs:\n    H = hess_fn(x)\n</code></pre> <p>Tip</p> <p>If your <code>coloring</code> looks wrong or overly dense, please help out <code>asdex</code>'s development by reporting it. These reports directly drive improvements and are one of the most impactful ways to contribute.</p>"},{"location":"how-to/hessians/#saving-and-loading-patterns","title":"Saving and Loading Patterns","text":"<p>Save a coloring to disk and reload it in a later session:</p> <pre><code>from asdex import hessian_coloring\n\ncoloring = hessian_coloring(g, input_shape=100)\ncoloring.save(\"colored.npz\")\n</code></pre> <pre><code>from asdex import ColoredPattern, hessian_from_coloring\n\ncoloring = ColoredPattern.load(\"colored.npz\")\nhess_fn = hessian_from_coloring(g, coloring)\n</code></pre> <p><code>SparsityPattern</code> supports the same <code>save</code>/<code>load</code> interface.</p>"},{"location":"how-to/hessians/#symmetric-coloring","title":"Symmetric Coloring","text":"<p>Hessians are symmetric (\\(H = H^\\top\\)), and <code>asdex</code> exploits this with star coloring (Gebremedhin et al., 2005). Symmetric coloring typically needs fewer colors than row or column coloring, since both \\(H_{ij}\\) and \\(H_{ji}\\) can be recovered from a single coloring.</p> <p>The convenience functions <code>hessian_coloring</code> and <code>hessian</code> use symmetric coloring automatically. Here we use the Rosenbrock function, a classic optimization benchmark whose Hessian is tridiagonal:</p> \\[f(x) = \\sum_{i=1}^{n-1} \\left[(1 - x_i)^2 + 100\\,(x_{i+1} - x_i^2)^2\\right]\\] <pre><code>import jax.numpy as jnp\nfrom asdex import hessian_coloring\n\ndef rosenbrock(x):\n    return jnp.sum((1 - x[:-1]) ** 2 + 100 * (x[1:] - x[:-1] ** 2) ** 2)\n\ncoloring = hessian_coloring(rosenbrock, input_shape=100)\n</code></pre> <pre><code>ColoredPattern(100\u00d7100, nnz=298, sparsity=97.0%, HVP, 3 colors)\n  3 HVPs (instead of 100 HVPs)\n\u23a1\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u2877\u2847\u23a4\n\u23a2\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2845\u23a5\n\u23a2\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ef\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2845\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ef\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2845\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28df\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u23a5   \u23a2\u28df\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u23a6   \u23a3\u28ff\u2803\u23a6\n</code></pre>"},{"location":"how-to/hessians/#separate-detection-and-coloring","title":"Separate Detection and Coloring","text":"<p>For even more control, you can split detection and coloring:</p> <pre><code>from asdex import hessian_sparsity, hessian_coloring_from_sparsity\n\nsparsity = hessian_sparsity(g, input_shape=100)\ncoloring = hessian_coloring_from_sparsity(sparsity)\n</code></pre> <p>Since the Hessian is the Jacobian of the gradient, <code>hessian_sparsity</code> simply calls <code>jacobian_sparsity(jax.grad(f), input_shape)</code>. The sparsity interpreter composes naturally with JAX's autodiff transforms.</p> <p>This is useful when you want to manually provide a sparsity pattern.</p>"},{"location":"how-to/hessians/#manually-providing-a-sparsity-pattern","title":"Manually Providing a Sparsity Pattern","text":"<p>You can provide a sparsity pattern manually if you already know it ahead of time. Create a <code>SparsityPattern</code> from coordinate arrays, a dense matrix, or a JAX BCOO matrix.</p> <p>From a dense boolean or numeric matrix:</p> <p><pre><code>import numpy as np\nfrom asdex import SparsityPattern\n\ndense = np.array([[1, 1, 0, 0],\n                  [1, 1, 1, 0],\n                  [0, 1, 1, 1],\n                  [0, 0, 1, 1]])\nsparsity = SparsityPattern.from_dense(dense)\n</code></pre> <pre><code>SparsityPattern(4\u00d74, nnz=10, sparsity=37.5%)\n\u25cf \u25cf \u22c5 \u22c5\n\u25cf \u25cf \u25cf \u22c5\n\u22c5 \u25cf \u25cf \u25cf\n\u22c5 \u22c5 \u25cf \u25cf\n</code></pre></p> <p>From row and column index arrays:</p> <p><pre><code>sparsity = SparsityPattern.from_coo(\n    rows=[0, 0, 1, 1, 1, 2, 2, 2, 3, 3],\n    cols=[0, 1, 0, 1, 2, 1, 2, 3, 2, 3],\n    shape=(4, 4),\n)\n</code></pre> <pre><code>SparsityPattern(4\u00d74, nnz=10, sparsity=37.5%)\n\u25cf \u25cf \u22c5 \u22c5\n\u25cf \u25cf \u25cf \u22c5\n\u22c5 \u25cf \u25cf \u25cf\n\u22c5 \u22c5 \u25cf \u25cf\n</code></pre></p> <p>From a JAX BCOO sparse matrix: <pre><code>sparsity = SparsityPattern.from_bcoo(bcoo_matrix)\n</code></pre></p> <p>Finally, color the sparsity pattern and compute the Hessian: <pre><code>from asdex import hessian_coloring_from_sparsity, hessian_from_coloring\n\ncoloring = hessian_coloring_from_sparsity(sparsity)\nhess_fn = hessian_from_coloring(f, coloring)\nH = hess_fn(x)\n</code></pre></p>"},{"location":"how-to/hessians/#choosing-an-hvp-mode","title":"Choosing an HVP Mode","text":"<p>By default, <code>hessian</code> uses forward-over-reverse AD to compute Hessian-vector products. You can select a different AD composition strategy via the <code>mode</code> parameter:</p> <pre><code>from asdex import hessian\n\nhess_fn_for = hessian(f, input_shape=100, mode=\"fwd_over_rev\")  # default\nhess_fn_rof = hessian(f, input_shape=100, mode=\"rev_over_fwd\")\nhess_fn_ror = hessian(f, input_shape=100, mode=\"rev_over_rev\")\n</code></pre> <p>All three modes produce the same mathematical result. They differ in how JAX's AD primitives are composed:</p> <ul> <li><code>fwd_over_rev</code> (default): <code>jvp(grad(f), ...)</code>.     Generally the fastest under JIT.</li> <li><code>rev_over_fwd</code>: <code>grad(lambda p: jvp(f, (p,), (v,))[1])</code>.     Can use less memory than forward-over-reverse for functions with many intermediates.</li> <li><code>rev_over_rev</code>: <code>grad(lambda y: dot(grad(f)(y), v))</code>.     Avoids forward-mode entirely;     useful when forward-mode is expensive or unsupported.</li> </ul> <p>Tip</p> <p>When in doubt, stick with the default <code>\"fwd_over_rev\"</code>. It is the most widely used and typically the most efficient under <code>jax.jit</code>.</p>"},{"location":"how-to/hessians/#verifying-results","title":"Verifying Results","text":"<p>Use <code>check_hessian_correctness</code> to verify <code>asdex</code>'s sparse Hessian against vanilla JAX.</p> <pre><code>from asdex import check_hessian_correctness, hessian_coloring\n\ncoloring = hessian_coloring(g, input_shape=x.shape)\ncheck_hessian_correctness(g, x, coloring)\n</code></pre> <p>Use verification for debugging and initial setup, not in production loops. A good place to call it is in your test suite.</p> <p>By default, this uses randomized matrix-vector products (<code>method=\"matvec\"</code>) to check the sparse Hessian against an HVP reference. The AD mode is derived from the coloring. This is cheap \u2014 O(k) in the number of probes \u2014 and scales to large problems. If the results match, the function returns silently. If they disagree, it raises a <code>VerificationError</code>.</p> <p>You can also set custom tolerances, the number of probes, and the PRNG seed:</p> <pre><code>check_hessian_correctness(g, x, coloring, rtol=1e-5, atol=1e-5, num_probes=50, seed=42)\n</code></pre> <p>For an exact element-wise comparison against the full dense Hessian, use <code>method=\"dense\"</code>:</p> <pre><code>check_hessian_correctness(g, x, coloring, method=\"dense\")\n</code></pre> <p>Dense computation</p> <p><code>method=\"dense\"</code> materializes the full dense Hessian, which is computationally very expensive for large problems.</p>"},{"location":"how-to/jacobians/","title":"Computing Sparse Jacobians","text":"<p><code>asdex</code> computes sparse Jacobians for functions \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) using row or column coloring with forward- or reverse-mode AD.</p> <p>Verify correctness at least once</p> <p>asdex's sparsity patterns should always be conservative, but a bug in sparsity detection could cause missing nonzeros. Always verify against vanilla JAX at least once on a new function. See Verifying Results below.</p>"},{"location":"how-to/jacobians/#basic-usage","title":"Basic Usage","text":"<p>Pass your function and its <code>input_shape</code> to <code>jacobian</code>:</p> <pre><code>from asdex import jacobian\n\njac_fn = jacobian(f, input_shape=1000)\nJ = jac_fn(x)\n</code></pre> <p>This runs the computationally expensive sparsity detection and coloring steps when defining <code>jac_fn</code>. Subsequent calls to <code>jac_fn</code> only need to perform the cheap decompression step. The result is a JAX BCOO sparse matrix.</p> <p>The same function can be reused across evaluations at different inputs:</p> <pre><code>for x in inputs:\n    J = jac_fn(x)\n</code></pre> <p><code>asdex</code> supports multi-dimensional input and output arrays. The Jacobian is always returned as a 2D matrix of shape \\((m, n)\\) where \\(n\\) is the total number of input elements and \\(m\\) is the total number of output elements</p>"},{"location":"how-to/jacobians/#precomputing-the-colored-pattern","title":"Precomputing the Colored Pattern","text":"<p>For more control, precompute the coloring explicitly:</p> <pre><code>from asdex import jacobian_coloring, jacobian_from_coloring\n\ndef f(x):\n    return (x[1:] - x[:-1]) ** 2\n\ncoloring = jacobian_coloring(f, input_shape=100)\n</code></pre> <pre><code>ColoredPattern(99\u00d7100, nnz=198, sparsity=98.0%, JVP, 2 colors)\n  2 JVPs (instead of 99 VJPs or 100 JVPs)\n\u23a1\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u28ff\u23a4\n\u23a2\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u23a6   \u23a3\u28ff\u23a6\n</code></pre> <p>This is useful when you want to visually inspect the coloring for correctness, or save it to disk to avoid recomputation. Pass the coloring to <code>jacobian_from_coloring</code> to compute the Jacobian:</p> <pre><code>jac_fn = jacobian_from_coloring(f, coloring)\n\nfor x in inputs:\n    J = jac_fn(x)\n</code></pre> <p>Tip</p> <p>If your <code>coloring</code> looks wrong or overly dense, please help out <code>asdex</code>'s development by reporting it. These reports directly drive improvements and are one of the most impactful ways to contribute.</p>"},{"location":"how-to/jacobians/#saving-and-loading-patterns","title":"Saving and Loading Patterns","text":"<p>Save a coloring to disk and reload it in a later session:</p> <pre><code>from asdex import jacobian_coloring\n\ncoloring = jacobian_coloring(f, input_shape=1000)\ncoloring.save(\"colored.npz\")\n</code></pre> <pre><code>from asdex import ColoredPattern, jacobian_from_coloring\n\ncoloring = ColoredPattern.load(\"colored.npz\")\njac_fn = jacobian_from_coloring(f, coloring)\n</code></pre> <p><code>SparsityPattern</code> supports the same <code>save</code>/<code>load</code> interface.</p>"},{"location":"how-to/jacobians/#choosing-row-vs-column-coloring","title":"Choosing Row vs Column Coloring","text":"<p>By default, <code>asdex</code> tries both row and column coloring and picks whichever needs fewer colors:</p> <pre><code>from asdex import jacobian_coloring\n\ndef f(x):\n    return (x[1:] - x[:-1]) ** 2\n\n# Automatic selection (default):\ncoloring = jacobian_coloring(f, input_shape=100)\n</code></pre> <pre><code>ColoredPattern(99\u00d7100, nnz=198, sparsity=98.0%, JVP, 2 colors)\n  2 JVPs (instead of 99 VJPs or 100 JVPs)\n\u23a1\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u28ff\u23a4\n\u23a2\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u23a6   \u23a3\u28ff\u23a6\n</code></pre> <p>You can also force a specific AD mode. <code>\"fwd\"</code> colors columns (uses JVPs, forward-mode AD), <code>\"rev\"</code> colors rows (uses VJPs, reverse-mode AD):</p> <pre><code># Force forward mode (column coloring, uses JVPs):\ncoloring = jacobian_coloring(f, input_shape=100, mode=\"fwd\")\n\n# Force reverse mode (row coloring, uses VJPs):\ncoloring = jacobian_coloring(f, input_shape=100, mode=\"rev\")\n</code></pre> <pre><code>ColoredPattern(99\u00d7100, nnz=198, sparsity=98.0%, VJP, 2 colors)\n  2 VJPs (instead of 99 VJPs or 100 JVPs)\n\u23a1\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4\n\u23a2\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u23a6\n                     \u2193\n\u23a1\u281a\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u2813\u23a4\n</code></pre> <p>The one-call <code>jacobian</code> API accepts the same <code>mode</code> parameter:</p> <pre><code>jac_fn = jacobian(f, input_shape=100, mode=\"rev\")\n</code></pre> <p>When the number of colors is equal, <code>asdex</code> prefers column coloring since JVPs are generally cheaper to compute in JAX.</p>"},{"location":"how-to/jacobians/#separate-detection-and-coloring","title":"Separate Detection and Coloring","text":"<p>For even more control, you can split detection and coloring:</p> <pre><code>from asdex import jacobian_sparsity, jacobian_coloring_from_sparsity\n\nsparsity = jacobian_sparsity(f, input_shape=1000)\ncoloring = jacobian_coloring_from_sparsity(sparsity, mode=\"fwd\")\n</code></pre> <p>This is useful when you want to manually provide a sparsity pattern.</p>"},{"location":"how-to/jacobians/#manually-providing-a-sparsity-pattern","title":"Manually Providing a Sparsity Pattern","text":"<p>You can provide a sparsity pattern manually if you already know it ahead of time. Create a <code>SparsityPattern</code> from coordinate arrays, a dense matrix, or a JAX BCOO matrix.</p> <p>From a dense boolean or numeric matrix:</p> <p><pre><code>import numpy as np\nfrom asdex import SparsityPattern\n\ndense = np.array([[1, 1, 0, 0],\n                  [0, 1, 1, 0],\n                  [0, 0, 1, 1]])\nsparsity = SparsityPattern.from_dense(dense)\n</code></pre> <pre><code>SparsityPattern(3\u00d74, nnz=6, sparsity=50.0%)\n\u25cf \u25cf \u22c5 \u22c5\n\u22c5 \u25cf \u25cf \u22c5\n\u22c5 \u22c5 \u25cf \u25cf\n</code></pre></p> <p>From row and column index arrays:</p> <p><pre><code>sparsity = SparsityPattern.from_coo(\n    rows=[0, 0, 1, 1, 2, 2],\n    cols=[0, 1, 1, 2, 2, 3],\n    shape=(3, 4),\n)\n</code></pre> <pre><code>SparsityPattern(3\u00d74, nnz=6, sparsity=50.0%)\n\u25cf \u25cf \u22c5 \u22c5\n\u22c5 \u25cf \u25cf \u22c5\n\u22c5 \u22c5 \u25cf \u25cf\n</code></pre></p> <p>From a JAX BCOO sparse matrix: <pre><code>sparsity = SparsityPattern.from_bcoo(bcoo_matrix)\n</code></pre></p> <p>Finally, color the sparsity pattern and compute the Jacobian: <pre><code>from asdex import jacobian_coloring_from_sparsity, jacobian_from_coloring\n\ncoloring = jacobian_coloring_from_sparsity(sparsity)\njac_fn = jacobian_from_coloring(f, coloring)\nJ = jac_fn(x)\n</code></pre></p>"},{"location":"how-to/jacobians/#verifying-results","title":"Verifying Results","text":"<p>Use <code>check_jacobian_correctness</code> to verify <code>asdex</code>'s sparse Jacobian against vanilla JAX.</p> <pre><code>from asdex import check_jacobian_correctness, jacobian_coloring\n\ncoloring = jacobian_coloring(f, input_shape=x.shape)\ncheck_jacobian_correctness(f, x, coloring)\n</code></pre> <p>Use verification for debugging and initial setup, not in production loops. A good place to call it is in your test suite.</p> <p>By default, this uses randomized matrix-vector products (<code>method=\"matvec\"</code>) to check the sparse Jacobian against JVPs or VJPs. The AD mode is derived from the coloring. This is cheap \u2014 O(k) in the number of probes \u2014 and scales to large problems. If the results match, the function returns silently. If they disagree, it raises a <code>VerificationError</code>.</p> <p>You can also set custom tolerances, the number of probes, and the PRNG seed:</p> <pre><code>check_jacobian_correctness(f, x, coloring, rtol=1e-5, atol=1e-5, num_probes=50, seed=42)\n</code></pre> <p>For an exact element-wise comparison against the full dense Jacobian, use <code>method=\"dense\"</code>:</p> <pre><code>check_jacobian_correctness(f, x, coloring, method=\"dense\")\n</code></pre> <p>Dense computation</p> <p><code>method=\"dense\"</code> materializes the full dense Jacobian, which is computationally very expensive for large problems.</p>"},{"location":"reference/","title":"Full API","text":""},{"location":"reference/#asdex.jacobian","title":"<code>asdex.jacobian(f, input_shape, *, mode=None, symmetric=False)</code>","text":"<p>Detect sparsity, color, and return a function computing sparse Jacobians.</p> <p>Combines <code>jacobian_coloring</code> and <code>jacobian_from_coloring</code> in one call.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Function taking an array and returning an array. Input and output may be multi-dimensional.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>JacobianMode | None</code> <p>AD mode. <code>\"fwd\"</code> uses JVPs (forward-mode AD), <code>\"rev\"</code> uses VJPs (reverse-mode AD). <code>None</code> picks whichever of fwd/rev needs fewer colors.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Requires a square Jacobian.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Jacobian as BCOO of shape <code>(m, n)</code> where <code>n = x.size</code> and <code>m = prod(output_shape)</code>.</p>"},{"location":"reference/#asdex.hessian","title":"<code>asdex.hessian(f, input_shape, *, mode=None, symmetric=True)</code>","text":"<p>Detect sparsity, color, and return a function computing sparse Hessians.</p> <p>Combines <code>hessian_coloring</code> and <code>hessian_from_coloring</code> in one call.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Scalar-valued function taking an array. Input may be multi-dimensional.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>HessianMode | None</code> <p>AD composition strategy for Hessian-vector products. <code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse. Defaults to <code>\"fwd_over_rev\"</code>.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Defaults to True (exploits H = H^T for fewer colors).</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Hessian as BCOO of shape <code>(n, n)</code> where <code>n = x.size</code>.</p>"},{"location":"reference/#asdex.jacobian_from_coloring","title":"<code>asdex.jacobian_from_coloring(f, coloring)</code>","text":"<p>Build a sparse Jacobian function from a pre-computed coloring.</p> <p>Uses row coloring + VJPs or column coloring + JVPs, depending on which needs fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Function taking an array and returning an array. Input and output may be multi-dimensional.</p> required <code>coloring</code> <code>ColoredPattern</code> <p>Pre-computed <code>ColoredPattern</code> from <code>jacobian_coloring</code>.</p> required <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Jacobian as BCOO of shape <code>(m, n)</code> where <code>n = x.size</code> and <code>m = prod(output_shape)</code>.</p>"},{"location":"reference/#asdex.hessian_from_coloring","title":"<code>asdex.hessian_from_coloring(f, coloring)</code>","text":"<p>Build a sparse Hessian function from a pre-computed coloring.</p> <p>Uses symmetric (star) coloring and Hessian-vector products by default.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Scalar-valued function taking an array. Input may be multi-dimensional.</p> required <code>coloring</code> <code>ColoredPattern</code> <p>Pre-computed <code>ColoredPattern</code> from <code>hessian_coloring</code>.</p> required <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Hessian as BCOO of shape <code>(n, n)</code> where <code>n = x.size</code>.</p>"},{"location":"reference/#asdex.jacobian_coloring","title":"<code>asdex.jacobian_coloring(f, input_shape, *, mode=None, symmetric=False)</code>","text":"<p>Detect Jacobian sparsity and color in one step.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>JacobianMode | None</code> <p>AD mode. <code>\"fwd\"</code> uses JVPs (forward-mode AD), <code>\"rev\"</code> uses VJPs (reverse-mode AD), <code>None</code> picks whichever of fwd/rev needs fewer colors (unless <code>symmetric</code> is True, in which case defaults to <code>\"fwd\"</code>).</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Requires a square Jacobian.</p> <code>False</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian_from_coloring</code>.</p>"},{"location":"reference/#asdex.hessian_coloring","title":"<code>asdex.hessian_coloring(f, input_shape, *, mode=None, symmetric=True)</code>","text":"<p>Detect Hessian sparsity and color in one step.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>HessianMode | None</code> <p>AD composition strategy for Hessian-vector products. <code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse. Defaults to <code>\"fwd_over_rev\"</code>.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Defaults to True (exploits H = H^T for fewer colors).</p> <code>True</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian_from_coloring</code>.</p>"},{"location":"reference/#asdex.jacobian_coloring_from_sparsity","title":"<code>asdex.jacobian_coloring_from_sparsity(sparsity, *, mode=None, symmetric=False)</code>","text":"<p>Color a sparsity pattern for sparse Jacobian computation.</p> <p>Assigns colors so that same-colored rows (or columns) can be computed together in a single VJP (or JVP).</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Sparsity pattern of shape (m, n).</p> required <code>mode</code> <code>JacobianMode | None</code> <p>AD mode. <code>\"fwd\"</code> uses JVPs (column coloring), <code>\"rev\"</code> uses VJPs (row coloring). <code>None</code> picks whichever of fwd/rev needs fewer colors (unless <code>symmetric</code> is True, in which case defaults to <code>\"fwd\"</code>).</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Requires a square pattern.</p> <code>False</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian_from_coloring</code>.</p>"},{"location":"reference/#asdex.hessian_coloring_from_sparsity","title":"<code>asdex.hessian_coloring_from_sparsity(sparsity, *, mode=None, symmetric=True)</code>","text":"<p>Color a sparsity pattern for sparse Hessian computation.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Sparsity pattern of shape (n, n).</p> required <code>mode</code> <code>HessianMode | None</code> <p>AD composition strategy for Hessian-vector products. <code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse. Defaults to <code>\"fwd_over_rev\"</code>.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Defaults to True (exploits Hessian symmetry for fewer colors).</p> <code>True</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian_from_coloring</code>.</p>"},{"location":"reference/#asdex.jacobian_sparsity","title":"<code>asdex.jacobian_sparsity(f, input_shape)</code>","text":"<p>Detect global Jacobian sparsity pattern for f: R^n -&gt; R^m.</p> <p>Analyzes the computation graph structure directly, without evaluating any derivatives. The result is valid for all inputs.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(m, n)</code> where <code>n = prod(input_shape)</code> and <code>m = prod(output_shape)</code>. Entry <code>(i, j)</code> is present if output <code>i</code> depends on input <code>j</code>.</p>"},{"location":"reference/#asdex.hessian_sparsity","title":"<code>asdex.hessian_sparsity(f, input_shape)</code>","text":"<p>Detect global Hessian sparsity pattern for f: R^n -&gt; R.</p> <p>Analyzes the Jacobian sparsity of the gradient function, without evaluating any derivatives. The result is valid for all inputs.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(n, n)</code> where <code>n = prod(input_shape)</code>. Entry <code>(i, j)</code> is present if <code>H[i, j]</code> may be nonzero.</p>"},{"location":"reference/#asdex.SparsityPattern","title":"<code>asdex.SparsityPattern</code>  <code>dataclass</code>","text":"<p>Sparse matrix pattern storing only structural information (no values).</p> <p>Stores row and column indices separately for efficient access by the coloring and decompression stages.</p> <p>Attributes:</p> Name Type Description <code>rows</code> <code>NDArray[int32]</code> <p>Row indices of non-zero entries, shape <code>(nnz,)</code></p> <code>cols</code> <code>NDArray[int32]</code> <p>Column indices of non-zero entries, shape <code>(nnz,)</code></p> <code>shape</code> <code>tuple[int, int]</code> <p>Matrix dimensions <code>(m, n)</code></p> <code>input_shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the function input that produced this pattern. Defaults to <code>(n,)</code> if not specified.</p>"},{"location":"reference/#asdex.SparsityPattern.nnz","title":"<code>nnz</code>  <code>property</code>","text":"<p>Number of non-zero elements.</p>"},{"location":"reference/#asdex.SparsityPattern.m","title":"<code>m</code>  <code>property</code>","text":"<p>Number of rows.</p>"},{"location":"reference/#asdex.SparsityPattern.n","title":"<code>n</code>  <code>property</code>","text":"<p>Number of columns.</p>"},{"location":"reference/#asdex.SparsityPattern.density","title":"<code>density</code>  <code>property</code>","text":"<p>Fraction of non-zero entries.</p>"},{"location":"reference/#asdex.SparsityPattern.col_to_rows","title":"<code>col_to_rows</code>  <code>cached</code> <code>property</code>","text":"<p>Mapping from column index to list of row indices with non-zeros.</p> <p>Used by the coloring algorithm to build the row conflict graph.</p>"},{"location":"reference/#asdex.SparsityPattern.row_to_cols","title":"<code>row_to_cols</code>  <code>cached</code> <code>property</code>","text":"<p>Mapping from row index to list of column indices with non-zeros.</p> <p>Used by the coloring algorithm to build the column conflict graph.</p>"},{"location":"reference/#asdex.SparsityPattern.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set defaults.</p>"},{"location":"reference/#asdex.SparsityPattern.from_coo","title":"<code>from_coo(rows, cols, shape, *, input_shape=None)</code>  <code>classmethod</code>","text":"<p>Create pattern from row and column index arrays.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>NDArray[int32] | list[int]</code> <p>Row indices of non-zero entries.</p> required <code>cols</code> <code>NDArray[int32] | list[int]</code> <p>Column indices of non-zero entries.</p> required <code>shape</code> <code>tuple[int, int]</code> <p>Matrix dimensions <code>(m, n)</code>.</p> required <code>input_shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the function input. Defaults to <code>(n,)</code> if not specified.</p> <code>None</code>"},{"location":"reference/#asdex.SparsityPattern.from_bcoo","title":"<code>from_bcoo(bcoo)</code>  <code>classmethod</code>","text":"<p>Create pattern from JAX BCOO sparse matrix.</p>"},{"location":"reference/#asdex.SparsityPattern.from_dense","title":"<code>from_dense(dense)</code>  <code>classmethod</code>","text":"<p>Create pattern from dense boolean/numeric matrix.</p> <p>Non-zero entries indicate pattern positions.</p>"},{"location":"reference/#asdex.SparsityPattern.to_bcoo","title":"<code>to_bcoo(data=None)</code>","text":"<p>Convert to JAX BCOO sparse matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray | None</code> <p>Optional data values. If None, uses all 1s.</p> <code>None</code>"},{"location":"reference/#asdex.SparsityPattern.todense","title":"<code>todense()</code>","text":"<p>Convert to dense numpy array with 1s at pattern positions.</p>"},{"location":"reference/#asdex.SparsityPattern.save","title":"<code>save(path)</code>","text":"<p>Save sparsity pattern to an <code>.npz</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike[str]</code> <p>Destination file path.</p> required"},{"location":"reference/#asdex.SparsityPattern.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load sparsity pattern from an <code>.npz</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike[str]</code> <p>Source file path.</p> required"},{"location":"reference/#asdex.SparsityPattern.__str__","title":"<code>__str__()</code>","text":"<p>Render sparsity pattern with header and dot/braille grid.</p>"},{"location":"reference/#asdex.SparsityPattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Return compact single-line representation.</p>"},{"location":"reference/#asdex.ColoredPattern","title":"<code>asdex.ColoredPattern</code>  <code>dataclass</code>","text":"<p>Result of a graph coloring for sparse differentiation.</p> <p>Attributes:</p> Name Type Description <code>sparsity</code> <code>SparsityPattern</code> <p>The sparsity pattern that was colored.</p> <code>colors</code> <code>NDArray[int32]</code> <p>Color assignment array. Shape <code>(m,)</code> for <code>\"rev\"</code> mode, <code>(n,)</code> for all other modes.</p> <code>num_colors</code> <code>int</code> <p>Total number of colors used.</p> <code>symmetric</code> <code>bool</code> <p>Whether symmetric (star) coloring was used.</p> <code>mode</code> <code>ColoringMode</code> <p>The AD mode. Resolved, never <code>\"auto\"</code>. <code>\"fwd\"</code> uses JVPs (forward-mode AD), <code>\"rev\"</code> uses VJPs (reverse-mode AD), <code>\"fwd_over_rev\"</code> uses forward-over-reverse HVPs, <code>\"rev_over_fwd\"</code> uses reverse-over-forward HVPs, <code>\"rev_over_rev\"</code> uses reverse-over-reverse HVPs.</p>"},{"location":"reference/#asdex.ColoredPattern.save","title":"<code>save(path)</code>","text":"<p>Save colored pattern to an <code>.npz</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike[str]</code> <p>Destination file path.</p> required"},{"location":"reference/#asdex.ColoredPattern.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load colored pattern from an <code>.npz</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike[str]</code> <p>Source file path.</p> required"},{"location":"reference/#asdex.ColoredPattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Return compact single-line representation.</p>"},{"location":"reference/#asdex.ColoredPattern.__str__","title":"<code>__str__()</code>","text":"<p>Render colored pattern with sparsity grid and color assignments.</p>"},{"location":"reference/#asdex.JacobianMode","title":"<code>asdex.JacobianMode = Literal['fwd', 'rev']</code>  <code>module-attribute</code>","text":"<p>AD mode for Jacobian computation.</p> <p><code>\"fwd\"</code> uses JVPs (forward-mode AD), <code>\"rev\"</code> uses VJPs (reverse-mode AD).</p>"},{"location":"reference/#asdex.HessianMode","title":"<code>asdex.HessianMode = Literal['fwd_over_rev', 'rev_over_fwd', 'rev_over_rev']</code>  <code>module-attribute</code>","text":"<p>AD composition strategy for Hessian-vector products.</p> <p><code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse.</p>"},{"location":"reference/coloring/","title":"Coloring","text":""},{"location":"reference/coloring/#asdex.jacobian_coloring","title":"<code>asdex.jacobian_coloring(f, input_shape, *, mode=None, symmetric=False)</code>","text":"<p>Detect Jacobian sparsity and color in one step.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>JacobianMode | None</code> <p>AD mode. <code>\"fwd\"</code> uses JVPs (forward-mode AD), <code>\"rev\"</code> uses VJPs (reverse-mode AD), <code>None</code> picks whichever of fwd/rev needs fewer colors (unless <code>symmetric</code> is True, in which case defaults to <code>\"fwd\"</code>).</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Requires a square Jacobian.</p> <code>False</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian_from_coloring</code>.</p>"},{"location":"reference/coloring/#asdex.hessian_coloring","title":"<code>asdex.hessian_coloring(f, input_shape, *, mode=None, symmetric=True)</code>","text":"<p>Detect Hessian sparsity and color in one step.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>HessianMode | None</code> <p>AD composition strategy for Hessian-vector products. <code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse. Defaults to <code>\"fwd_over_rev\"</code>.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Defaults to True (exploits H = H^T for fewer colors).</p> <code>True</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian_from_coloring</code>.</p>"},{"location":"reference/coloring/#asdex.jacobian_coloring_from_sparsity","title":"<code>asdex.jacobian_coloring_from_sparsity(sparsity, *, mode=None, symmetric=False)</code>","text":"<p>Color a sparsity pattern for sparse Jacobian computation.</p> <p>Assigns colors so that same-colored rows (or columns) can be computed together in a single VJP (or JVP).</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Sparsity pattern of shape (m, n).</p> required <code>mode</code> <code>JacobianMode | None</code> <p>AD mode. <code>\"fwd\"</code> uses JVPs (column coloring), <code>\"rev\"</code> uses VJPs (row coloring). <code>None</code> picks whichever of fwd/rev needs fewer colors (unless <code>symmetric</code> is True, in which case defaults to <code>\"fwd\"</code>).</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Requires a square pattern.</p> <code>False</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian_from_coloring</code>.</p>"},{"location":"reference/coloring/#asdex.hessian_coloring_from_sparsity","title":"<code>asdex.hessian_coloring_from_sparsity(sparsity, *, mode=None, symmetric=True)</code>","text":"<p>Color a sparsity pattern for sparse Hessian computation.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Sparsity pattern of shape (n, n).</p> required <code>mode</code> <code>HessianMode | None</code> <p>AD composition strategy for Hessian-vector products. <code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse. Defaults to <code>\"fwd_over_rev\"</code>.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Defaults to True (exploits Hessian symmetry for fewer colors).</p> <code>True</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian_from_coloring</code>.</p>"},{"location":"reference/data-structures/","title":"Data Structures","text":""},{"location":"reference/data-structures/#asdex.ColoredPattern","title":"<code>asdex.ColoredPattern</code>  <code>dataclass</code>","text":"<p>Result of a graph coloring for sparse differentiation.</p> <p>Attributes:</p> Name Type Description <code>sparsity</code> <code>SparsityPattern</code> <p>The sparsity pattern that was colored.</p> <code>colors</code> <code>NDArray[int32]</code> <p>Color assignment array. Shape <code>(m,)</code> for <code>\"rev\"</code> mode, <code>(n,)</code> for all other modes.</p> <code>num_colors</code> <code>int</code> <p>Total number of colors used.</p> <code>symmetric</code> <code>bool</code> <p>Whether symmetric (star) coloring was used.</p> <code>mode</code> <code>ColoringMode</code> <p>The AD mode. Resolved, never <code>\"auto\"</code>. <code>\"fwd\"</code> uses JVPs (forward-mode AD), <code>\"rev\"</code> uses VJPs (reverse-mode AD), <code>\"fwd_over_rev\"</code> uses forward-over-reverse HVPs, <code>\"rev_over_fwd\"</code> uses reverse-over-forward HVPs, <code>\"rev_over_rev\"</code> uses reverse-over-reverse HVPs.</p>"},{"location":"reference/data-structures/#asdex.ColoredPattern.save","title":"<code>save(path)</code>","text":"<p>Save colored pattern to an <code>.npz</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike[str]</code> <p>Destination file path.</p> required"},{"location":"reference/data-structures/#asdex.ColoredPattern.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load colored pattern from an <code>.npz</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike[str]</code> <p>Source file path.</p> required"},{"location":"reference/data-structures/#asdex.ColoredPattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Return compact single-line representation.</p>"},{"location":"reference/data-structures/#asdex.ColoredPattern.__str__","title":"<code>__str__()</code>","text":"<p>Render colored pattern with sparsity grid and color assignments.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern","title":"<code>asdex.SparsityPattern</code>  <code>dataclass</code>","text":"<p>Sparse matrix pattern storing only structural information (no values).</p> <p>Stores row and column indices separately for efficient access by the coloring and decompression stages.</p> <p>Attributes:</p> Name Type Description <code>rows</code> <code>NDArray[int32]</code> <p>Row indices of non-zero entries, shape <code>(nnz,)</code></p> <code>cols</code> <code>NDArray[int32]</code> <p>Column indices of non-zero entries, shape <code>(nnz,)</code></p> <code>shape</code> <code>tuple[int, int]</code> <p>Matrix dimensions <code>(m, n)</code></p> <code>input_shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the function input that produced this pattern. Defaults to <code>(n,)</code> if not specified.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.nnz","title":"<code>nnz</code>  <code>property</code>","text":"<p>Number of non-zero elements.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.m","title":"<code>m</code>  <code>property</code>","text":"<p>Number of rows.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.n","title":"<code>n</code>  <code>property</code>","text":"<p>Number of columns.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.density","title":"<code>density</code>  <code>property</code>","text":"<p>Fraction of non-zero entries.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.col_to_rows","title":"<code>col_to_rows</code>  <code>cached</code> <code>property</code>","text":"<p>Mapping from column index to list of row indices with non-zeros.</p> <p>Used by the coloring algorithm to build the row conflict graph.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.row_to_cols","title":"<code>row_to_cols</code>  <code>cached</code> <code>property</code>","text":"<p>Mapping from row index to list of column indices with non-zeros.</p> <p>Used by the coloring algorithm to build the column conflict graph.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set defaults.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.from_coo","title":"<code>from_coo(rows, cols, shape, *, input_shape=None)</code>  <code>classmethod</code>","text":"<p>Create pattern from row and column index arrays.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>NDArray[int32] | list[int]</code> <p>Row indices of non-zero entries.</p> required <code>cols</code> <code>NDArray[int32] | list[int]</code> <p>Column indices of non-zero entries.</p> required <code>shape</code> <code>tuple[int, int]</code> <p>Matrix dimensions <code>(m, n)</code>.</p> required <code>input_shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the function input. Defaults to <code>(n,)</code> if not specified.</p> <code>None</code>"},{"location":"reference/data-structures/#asdex.SparsityPattern.from_bcoo","title":"<code>from_bcoo(bcoo)</code>  <code>classmethod</code>","text":"<p>Create pattern from JAX BCOO sparse matrix.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.from_dense","title":"<code>from_dense(dense)</code>  <code>classmethod</code>","text":"<p>Create pattern from dense boolean/numeric matrix.</p> <p>Non-zero entries indicate pattern positions.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.to_bcoo","title":"<code>to_bcoo(data=None)</code>","text":"<p>Convert to JAX BCOO sparse matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray | None</code> <p>Optional data values. If None, uses all 1s.</p> <code>None</code>"},{"location":"reference/data-structures/#asdex.SparsityPattern.todense","title":"<code>todense()</code>","text":"<p>Convert to dense numpy array with 1s at pattern positions.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.save","title":"<code>save(path)</code>","text":"<p>Save sparsity pattern to an <code>.npz</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike[str]</code> <p>Destination file path.</p> required"},{"location":"reference/data-structures/#asdex.SparsityPattern.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load sparsity pattern from an <code>.npz</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike[str]</code> <p>Source file path.</p> required"},{"location":"reference/data-structures/#asdex.SparsityPattern.__str__","title":"<code>__str__()</code>","text":"<p>Render sparsity pattern with header and dot/braille grid.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Return compact single-line representation.</p>"},{"location":"reference/hessian/","title":"Hessian","text":""},{"location":"reference/hessian/#asdex.hessian","title":"<code>asdex.hessian(f, input_shape, *, mode=None, symmetric=True)</code>","text":"<p>Detect sparsity, color, and return a function computing sparse Hessians.</p> <p>Combines <code>hessian_coloring</code> and <code>hessian_from_coloring</code> in one call.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Scalar-valued function taking an array. Input may be multi-dimensional.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>HessianMode | None</code> <p>AD composition strategy for Hessian-vector products. <code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse. Defaults to <code>\"fwd_over_rev\"</code>.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Defaults to True (exploits H = H^T for fewer colors).</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Hessian as BCOO of shape <code>(n, n)</code> where <code>n = x.size</code>.</p>"},{"location":"reference/hessian/#asdex.hessian_from_coloring","title":"<code>asdex.hessian_from_coloring(f, coloring)</code>","text":"<p>Build a sparse Hessian function from a pre-computed coloring.</p> <p>Uses symmetric (star) coloring and Hessian-vector products by default.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Scalar-valued function taking an array. Input may be multi-dimensional.</p> required <code>coloring</code> <code>ColoredPattern</code> <p>Pre-computed <code>ColoredPattern</code> from <code>hessian_coloring</code>.</p> required <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Hessian as BCOO of shape <code>(n, n)</code> where <code>n = x.size</code>.</p>"},{"location":"reference/hessian/#asdex.hessian_coloring","title":"<code>asdex.hessian_coloring(f, input_shape, *, mode=None, symmetric=True)</code>","text":"<p>Detect Hessian sparsity and color in one step.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>HessianMode | None</code> <p>AD composition strategy for Hessian-vector products. <code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse. Defaults to <code>\"fwd_over_rev\"</code>.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Defaults to True (exploits H = H^T for fewer colors).</p> <code>True</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian_from_coloring</code>.</p>"},{"location":"reference/hessian/#asdex.hessian_coloring_from_sparsity","title":"<code>asdex.hessian_coloring_from_sparsity(sparsity, *, mode=None, symmetric=True)</code>","text":"<p>Color a sparsity pattern for sparse Hessian computation.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Sparsity pattern of shape (n, n).</p> required <code>mode</code> <code>HessianMode | None</code> <p>AD composition strategy for Hessian-vector products. <code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse. Defaults to <code>\"fwd_over_rev\"</code>.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Defaults to True (exploits Hessian symmetry for fewer colors).</p> <code>True</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian_from_coloring</code>.</p>"},{"location":"reference/hessian/#asdex.hessian_sparsity","title":"<code>asdex.hessian_sparsity(f, input_shape)</code>","text":"<p>Detect global Hessian sparsity pattern for f: R^n -&gt; R.</p> <p>Analyzes the Jacobian sparsity of the gradient function, without evaluating any derivatives. The result is valid for all inputs.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(n, n)</code> where <code>n = prod(input_shape)</code>. Entry <code>(i, j)</code> is present if <code>H[i, j]</code> may be nonzero.</p>"},{"location":"reference/hessian/#asdex.check_hessian_correctness","title":"<code>asdex.check_hessian_correctness(f, x, coloring, *, method='matvec', num_probes=25, seed=0, rtol=None, atol=None)</code>","text":"<p>Verify asdex's sparse Hessian against a JAX reference at a given input.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Scalar-valued function taking an array.</p> required <code>x</code> <code>ArrayLike</code> <p>Input at which to evaluate the Hessian.</p> required <code>coloring</code> <code>ColoredPattern</code> <p>Pre-computed colored pattern from :func:<code>~asdex.hessian_coloring</code>.</p> required <code>method</code> <code>Literal['matvec', 'dense']</code> <p>Verification method. <code>\"matvec\"</code> uses randomized matrix-vector products, which is O(k) in the number of probes. <code>\"dense\"</code> materializes the full dense Hessian, which is O(n^2).</p> <code>'matvec'</code> <code>num_probes</code> <code>int</code> <p>Number of random probe vectors (only used by <code>\"matvec\"</code>).</p> <code>25</code> <code>seed</code> <code>int</code> <p>PRNG seed for reproducibility (only used by <code>\"matvec\"</code>).</p> <code>0</code> <code>rtol</code> <code>float | None</code> <p>Relative tolerance for comparison. Defaults to 1e-5 for <code>\"matvec\"</code> and 1e-7 for <code>\"dense\"</code>.</p> <code>None</code> <code>atol</code> <code>float | None</code> <p>Absolute tolerance for comparison. Defaults to 1e-5 for <code>\"matvec\"</code> and 1e-7 for <code>\"dense\"</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>VerificationError</code> <p>If the sparse and reference Hessians disagree.</p>"},{"location":"reference/hessian/#asdex.HessianMode","title":"<code>asdex.HessianMode = Literal['fwd_over_rev', 'rev_over_fwd', 'rev_over_rev']</code>  <code>module-attribute</code>","text":"<p>AD composition strategy for Hessian-vector products.</p> <p><code>\"fwd_over_rev\"</code> uses forward-over-reverse, <code>\"rev_over_fwd\"</code> uses reverse-over-forward, <code>\"rev_over_rev\"</code> uses reverse-over-reverse.</p>"},{"location":"reference/jacobian/","title":"Jacobian","text":""},{"location":"reference/jacobian/#asdex.jacobian","title":"<code>asdex.jacobian(f, input_shape, *, mode=None, symmetric=False)</code>","text":"<p>Detect sparsity, color, and return a function computing sparse Jacobians.</p> <p>Combines <code>jacobian_coloring</code> and <code>jacobian_from_coloring</code> in one call.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Function taking an array and returning an array. Input and output may be multi-dimensional.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>JacobianMode | None</code> <p>AD mode. <code>\"fwd\"</code> uses JVPs (forward-mode AD), <code>\"rev\"</code> uses VJPs (reverse-mode AD). <code>None</code> picks whichever of fwd/rev needs fewer colors.</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Requires a square Jacobian.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Jacobian as BCOO of shape <code>(m, n)</code> where <code>n = x.size</code> and <code>m = prod(output_shape)</code>.</p>"},{"location":"reference/jacobian/#asdex.jacobian_from_coloring","title":"<code>asdex.jacobian_from_coloring(f, coloring)</code>","text":"<p>Build a sparse Jacobian function from a pre-computed coloring.</p> <p>Uses row coloring + VJPs or column coloring + JVPs, depending on which needs fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Function taking an array and returning an array. Input and output may be multi-dimensional.</p> required <code>coloring</code> <code>ColoredPattern</code> <p>Pre-computed <code>ColoredPattern</code> from <code>jacobian_coloring</code>.</p> required <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Jacobian as BCOO of shape <code>(m, n)</code> where <code>n = x.size</code> and <code>m = prod(output_shape)</code>.</p>"},{"location":"reference/jacobian/#asdex.jacobian_coloring","title":"<code>asdex.jacobian_coloring(f, input_shape, *, mode=None, symmetric=False)</code>","text":"<p>Detect Jacobian sparsity and color in one step.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>mode</code> <code>JacobianMode | None</code> <p>AD mode. <code>\"fwd\"</code> uses JVPs (forward-mode AD), <code>\"rev\"</code> uses VJPs (reverse-mode AD), <code>None</code> picks whichever of fwd/rev needs fewer colors (unless <code>symmetric</code> is True, in which case defaults to <code>\"fwd\"</code>).</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Requires a square Jacobian.</p> <code>False</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian_from_coloring</code>.</p>"},{"location":"reference/jacobian/#asdex.jacobian_coloring_from_sparsity","title":"<code>asdex.jacobian_coloring_from_sparsity(sparsity, *, mode=None, symmetric=False)</code>","text":"<p>Color a sparsity pattern for sparse Jacobian computation.</p> <p>Assigns colors so that same-colored rows (or columns) can be computed together in a single VJP (or JVP).</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Sparsity pattern of shape (m, n).</p> required <code>mode</code> <code>JacobianMode | None</code> <p>AD mode. <code>\"fwd\"</code> uses JVPs (column coloring), <code>\"rev\"</code> uses VJPs (row coloring). <code>None</code> picks whichever of fwd/rev needs fewer colors (unless <code>symmetric</code> is True, in which case defaults to <code>\"fwd\"</code>).</p> <code>None</code> <code>symmetric</code> <code>bool</code> <p>Whether to use symmetric (star) coloring. Requires a square pattern.</p> <code>False</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian_from_coloring</code>.</p>"},{"location":"reference/jacobian/#asdex.jacobian_sparsity","title":"<code>asdex.jacobian_sparsity(f, input_shape)</code>","text":"<p>Detect global Jacobian sparsity pattern for f: R^n -&gt; R^m.</p> <p>Analyzes the computation graph structure directly, without evaluating any derivatives. The result is valid for all inputs.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(m, n)</code> where <code>n = prod(input_shape)</code> and <code>m = prod(output_shape)</code>. Entry <code>(i, j)</code> is present if output <code>i</code> depends on input <code>j</code>.</p>"},{"location":"reference/jacobian/#asdex.check_jacobian_correctness","title":"<code>asdex.check_jacobian_correctness(f, x, coloring, *, method='matvec', num_probes=25, seed=0, rtol=None, atol=None)</code>","text":"<p>Verify asdex's sparse Jacobian against a JAX reference at a given input.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Function taking an array and returning an array.</p> required <code>x</code> <code>ArrayLike</code> <p>Input at which to evaluate the Jacobian.</p> required <code>coloring</code> <code>ColoredPattern</code> <p>Pre-computed colored pattern from :func:<code>~asdex.jacobian_coloring</code>.</p> required <code>method</code> <code>Literal['matvec', 'dense']</code> <p>Verification method. <code>\"matvec\"</code> uses randomized matrix-vector products, which is O(k) in the number of probes. <code>\"dense\"</code> materializes the full dense Jacobian, which is O(n^2).</p> <code>'matvec'</code> <code>num_probes</code> <code>int</code> <p>Number of random probe vectors (only used by <code>\"matvec\"</code>).</p> <code>25</code> <code>seed</code> <code>int</code> <p>PRNG seed for reproducibility (only used by <code>\"matvec\"</code>).</p> <code>0</code> <code>rtol</code> <code>float | None</code> <p>Relative tolerance for comparison. Defaults to 1e-5 for <code>\"matvec\"</code> and 1e-7 for <code>\"dense\"</code>.</p> <code>None</code> <code>atol</code> <code>float | None</code> <p>Absolute tolerance for comparison. Defaults to 1e-5 for <code>\"matvec\"</code> and 1e-7 for <code>\"dense\"</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>VerificationError</code> <p>If the sparse and reference Jacobians disagree.</p>"},{"location":"reference/jacobian/#asdex.VerificationError","title":"<code>asdex.VerificationError</code>","text":"<p>               Bases: <code>AssertionError</code></p> <p>Raised when asdex's sparse result does not match JAX's dense reference.</p> <p>This indicates that the detected sparsity pattern is missing nonzeros, which is a bug \u2014 asdex's patterns should always be conservative (i.e., contain at least all true nonzeros). If you encounter this error, please help out asdex's development by reporting this at https://github.com/adrhill/asdex/issues.</p>"},{"location":"reference/jacobian/#asdex.JacobianMode","title":"<code>asdex.JacobianMode = Literal['fwd', 'rev']</code>  <code>module-attribute</code>","text":"<p>AD mode for Jacobian computation.</p> <p><code>\"fwd\"</code> uses JVPs (forward-mode AD), <code>\"rev\"</code> uses VJPs (reverse-mode AD).</p>"},{"location":"reference/sparsity/","title":"Sparsity Detection","text":""},{"location":"reference/sparsity/#asdex.jacobian_sparsity","title":"<code>asdex.jacobian_sparsity(f, input_shape)</code>","text":"<p>Detect global Jacobian sparsity pattern for f: R^n -&gt; R^m.</p> <p>Analyzes the computation graph structure directly, without evaluating any derivatives. The result is valid for all inputs.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(m, n)</code> where <code>n = prod(input_shape)</code> and <code>m = prod(output_shape)</code>. Entry <code>(i, j)</code> is present if output <code>i</code> depends on input <code>j</code>.</p>"},{"location":"reference/sparsity/#asdex.hessian_sparsity","title":"<code>asdex.hessian_sparsity(f, input_shape)</code>","text":"<p>Detect global Hessian sparsity pattern for f: R^n -&gt; R.</p> <p>Analyzes the Jacobian sparsity of the gradient function, without evaluating any derivatives. The result is valid for all inputs.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(n, n)</code> where <code>n = prod(input_shape)</code>. Entry <code>(i, j)</code> is present if <code>H[i, j]</code> may be nonzero.</p>"},{"location":"tutorials/getting-started/","title":"Getting Started","text":"<p>This tutorial walks through the three stages of automatic sparse differentiation: detection, coloring, and decompression.</p>"},{"location":"tutorials/getting-started/#the-problem","title":"The Problem","text":"<p>For a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), computing the full Jacobian \\(J \\in \\mathbb{R}^{m \\times n}\\) requires \\(n\\) forward-mode or \\(m\\) reverse-mode AD passes. In practice, for example in scientific machine learning, many Jacobians are sparse (i.e., most entries are structurally zero, regardless of the input).</p> <p><code>asdex</code> exploits this sparsity in three steps: 1. Detect the sparsity pattern by tracing the computation graph 2. Color the pattern so that structurally orthogonal rows (or columns) share a color 3. Decompress one AD pass per color into the sparse Jacobian or Hessian</p> <p>This reduces the computational cost from \\(m\\) (or \\(n\\)) AD passes to just the number of colors, yielding significant speedups on large sparse problems, especially when the cost of detection and coloring can be amortized over repeated evaluations. The same approach applies to sparse Hessians via forward-over-reverse AD.</p>"},{"location":"tutorials/getting-started/#sparse-jacobians","title":"Sparse Jacobians","text":"<p>Consider the squared differences function \\(f(x)_i = (x_{i+1} - x_i)^2\\), which has a banded Jacobian. Detect the sparsity pattern and color it in one step:</p> <pre><code>import jax.numpy as jnp\nfrom asdex import jacobian_coloring\n\ndef f(x):\n    return (x[1:] - x[:-1]) ** 2\n\nx = jnp.ones(50)\n\ncoloring = jacobian_coloring(f, input_shape=x.shape)\n</code></pre> <pre><code>ColoredPattern(49\u00d750, nnz=98, sparsity=96.0%, JVP, 2 colors)\n  2 JVPs (instead of 49 VJPs or 50 JVPs)\n\u23a1\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u28ff\u23a4\n\u23a2\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u23a5   \u23a2\u28ff\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2809\u23a6   \u23a3\u2809\u23a6\n</code></pre> <p>The print-out shows the original sparsity pattern (left) compressed into just two colors (right). <code>asdex</code> automatically ran multiple coloring algorithms, selected column coloring (2 JVPs) over row coloring (2 VJPs) since JVPs are cheaper, reducing the cost from 49 VJPs or 50 JVPs without coloring to just 2 JVPs. Note that on small problems, this doesn't directly translate into a speedup of factor 25x, as the decompression overhead dominates.</p> <p>Global Sparsity Patterns</p> <p>The detected pattern is a global sparsity pattern: it depends only on the function's structure, not on any particular input. This means it may contain extra nonzeros compared to the sparsity at a specific point, but it is guaranteed to be correct everywhere and can therefore be reused.</p> <p>If you encounter overly conservative patterns, please open an issue. These reports directly drive improvements and are one of the most impactful ways to contribute.</p> <p>Now we can compute the sparse Jacobian using the coloring:</p> <pre><code>from asdex import jacobian_from_coloring\n\njac_fn = jacobian_from_coloring(f, coloring)\nJ = jac_fn(x)\n</code></pre> <p>The result is a JAX BCOO sparse matrix. We can verify that <code>asdex</code> produces the same result as <code>jax.jacobian</code>:</p> <pre><code>import jax\nimport numpy as np\n\nJ_asdex = J.todense()\nJ_jax = jax.jacobian(f)(x)\n\nnp.testing.assert_allclose(J_asdex, J_jax, atol=1e-6)\n</code></pre> <p><code>asdex</code> also provides <code>check_jacobian_correctness</code> as a convenience for this comparison \u2014 see Verifying Results.</p> <p>On larger problems, the speedup from coloring becomes significant. Let's benchmark on a 5000-dimensional input (note that timings may vary as part of the doc-building process). This time, we use <code>asdex.jacobian</code>, which calls <code>jacobian_coloring</code> and <code>jacobian_from_coloring</code>:</p> <pre><code>import asdex\nimport jax\nimport timeit\n\nn = 5000\nx = jnp.ones(n)\n\njac_fn_asdex = asdex.jacobian(f, input_shape=n)\njac_fn_jax = jax.jacobian(f)\n\n# Warm up JIT caches\n_ = jac_fn_asdex(x)\n_ = jac_fn_jax(x)\n\nt_asdex = timeit.timeit(lambda: jac_fn_asdex(x).block_until_ready(), number=10) / 10\nt_jax = timeit.timeit(lambda: jac_fn_jax(x).block_until_ready(), number=10) / 10\n</code></pre> <pre><code>asdex.jacobian:      8.46 ms\njax.jacobian:      217.15 ms\nspeedup:             25.7x\n</code></pre> <p>Precompute for Repeated Evaluations</p> <p>The coloring depends only on the function structure, not the input values. When computing Jacobians at many different inputs, precompute the coloring once and reuse it:</p> <pre><code>jac_fn = jacobian(f, input_shape=5000)\n\nfor x in inputs:\n    J = jac_fn(x)\n</code></pre>"},{"location":"tutorials/getting-started/#sparse-hessians","title":"Sparse Hessians","text":"<p>For scalar-valued functions \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\), <code>asdex</code> can detect Hessian sparsity and compute sparse Hessians:</p> <pre><code>from asdex import hessian\n\ndef g(x):\n    return jnp.sum(x ** 2)\n\nhess_fn = hessian(g, input_shape=20)\n\nfor x in inputs:\n    H = hess_fn(x)\n</code></pre>"},{"location":"tutorials/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Computing Sparse Jacobians \u2014 Guide on Jacobian computation</li> <li>Computing Sparse Hessians \u2014 Guide on Hessian computation</li> <li>Sparsity Detection \u2014 Explanation how sparsity patterns are detected</li> <li>Graph Coloring \u2014 Explanation how coloring reduces cost</li> </ul>"}]}