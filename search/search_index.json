{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"asdex","text":"<p>Automatic Sparse Differentiation in JAX.</p> <p><code>asdex</code> (rumored to be pronounced Aztecs) exploits sparsity structure to efficiently compute sparse Jacobians and Hessians. It implements a custom Jaxpr interpreter that detects sparsity patterns from the computation graph, then uses graph coloring to minimize the number of AD passes needed. Refer to our Illustrated Guide to Automatic Sparse Differentiation for more information.</p> <p>Alpha Software</p> <p><code>asdex</code> is in early development. The API may change without notice. Use at your own risk.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/adrhill/asdex.git\n</code></pre> <p>Or with uv:</p> <pre><code>uv add git+https://github.com/adrhill/asdex.git\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import numpy as np\nfrom asdex import jacobian_coloring, jacobian\n\ndef f(x):\n    return (x[1:] - x[:-1]) ** 2\n\n# Detect sparsity and color in one step:\ncolored_pattern = jacobian_coloring(f, input_shape=1000)\n\n# Compute sparse Jacobians efficiently:\nx = np.random.randn(1000)\njac_fn = jacobian(f, colored_pattern)\nJ = jac_fn(x)\n</code></pre> <p>Instead of 999 VJPs or 1000 JVPs, <code>asdex</code> computes the full sparse Jacobian with just 2 JVPs.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started \u2014 step-by-step tutorial</li> <li>How-To Guides \u2014 task-oriented recipes</li> <li>Explanation \u2014 how and why it works</li> <li>API Reference \u2014 full API documentation</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This package is built with Claude Code based on previous work by Adrian Hill, Guillaume Dalle, and Alexis Montoison in the Julia programming language:</p> <ul> <li>An Illustrated Guide to Automatic Sparse Differentiation, A. Hill, G. Dalle, A. Montoison (2025)</li> <li>Sparser, Better, Faster, Stronger: Efficient Automatic Differentiation for Sparse Jacobians and Hessians, A. Hill &amp; G. Dalle (2025)</li> <li>Revisiting Sparse Matrix Coloring and Bicoloring, A. Montoison, G. Dalle, A. Gebremedhin (2025)</li> <li>SparseConnectivityTracer.jl, A. Hill, G. Dalle</li> <li>SparseMatrixColorings.jl, G. Dalle, A. Montoison</li> <li>sparsediffax, G. Dalle</li> </ul> <p>which in turn stands on the shoulders of giants \u2014 notably Andreas Griewank, Andrea Walther, and Assefaw Gebremedhin.</p>"},{"location":"explanation/coloring/","title":"Graph Coloring","text":"<p>Graph coloring is the key technique that makes sparse differentiation efficient. This page explains why coloring reduces the cost of computing sparse Jacobians and Hessians, how rows or columns are grouped to share AD passes, and how the results are compressed and decompressed.</p>"},{"location":"explanation/coloring/#why-coloring-helps","title":"Why Coloring Helps","text":"<p>Consider a Jacobian with \\(m\\) rows and \\(n\\) columns. Without exploiting sparsity, computing it requires \\(m\\) VJPs (one per row) or \\(n\\) JVPs (one per column). But if two columns have no nonzero rows in common \u2014 that is, they are structurally orthogonal \u2014 their JVPs can be combined into a single pass. We simply add their seed vectors together and decompose the result afterward, which works because the nonzeros don't overlap. The same idea applies symmetrically to rows and VJPs.</p> <p>To see this concretely, consider a \\(4 \\times 6\\) Jacobian with the following sparsity pattern (\\(\\times\\) marks a structural nonzero):</p> \\[ J = \\begin{pmatrix} \\times &amp; \\times &amp; &amp; &amp; &amp; \\\\ &amp; &amp; \\times &amp; \\times &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\times &amp; \\times \\\\ \\times &amp; &amp; &amp; \\times &amp; &amp; \\\\ \\end{pmatrix} \\] <p>Computing \\(J\\) by forward-mode AD would naively require 6 JVPs, one per column. But inspecting the sparsity pattern reveals two groups of structurally orthogonal columns: \\(\\{1, 3, 5\\}\\) and \\(\\{2, 4, 6\\}\\). Assigning one color per group reduces 6 JVPs to just 2.</p>"},{"location":"explanation/coloring/#the-conflict-graph","title":"The Conflict Graph","text":"<p>Graph coloring formalizes this idea. We build a conflict graph whose vertices are the rows (or columns) of the matrix and whose edges connect pairs that share a nonzero column (or row). A proper coloring of this graph assigns colors to vertices so that no two adjacent vertices share a color. Vertices with the same color are then guaranteed to be structurally orthogonal, meaning they can share an AD pass. The total number of passes equals the number of colors, which is often dramatically fewer than the matrix dimension.</p> <p>There are two variants. Row coloring treats rows as vertices and connects rows that share a nonzero column; same-colored rows are evaluated together using VJPs (reverse-mode AD). Column coloring treats columns as vertices and connects columns that share a nonzero row; same-colored columns are evaluated together using JVPs (forward-mode AD). By default, <code>asdex</code> tries both and picks whichever needs fewer colors. When tied, it prefers column coloring since JVPs are generally cheaper in JAX.</p>"},{"location":"explanation/coloring/#compression-and-decompression","title":"Compression and Decompression","text":"<p>Coloring defines not just how many AD passes to perform, but also how to set up each pass and how to extract the sparse matrix from the results.</p> <p>The coloring assigns each column (or row) a color, and from this we build a seed matrix \\(S\\) with one column per color. The seed for color \\(c\\) is the sum of the standard basis vectors for all columns assigned that color. In the example above, the two seeds are \\(e_1 + e_3 + e_5\\) and \\(e_2 + e_4 + e_6\\). Running one JVP per seed produces the compressed matrix \\(B = JS\\), which has only <code>num_colors</code> columns instead of \\(n\\).</p> <p>Recovering the sparse Jacobian from \\(B\\) is called decompression. Because same-colored columns are structurally orthogonal, each nonzero \\(J_{ij}\\) appears unambiguously in exactly one entry of \\(B\\): row \\(i\\), column \\(\\text{color}(j)\\). We simply read off each nonzero from the compressed matrix using the known color assignments \u2014 this is direct decompression. <code>asdex</code> uses direct decompression exclusively. An alternative family of methods (substitution-based decompression) solves small triangular systems to recover entries from denser colorings, but <code>asdex</code> does not implement these.</p>"},{"location":"explanation/coloring/#symmetric-coloring-for-hessians","title":"Symmetric Coloring for Hessians","text":"<p>Hessians are symmetric (\\(H_{ij} = H_{ji}\\)), so each off-diagonal entry appears twice in the matrix. Exploiting this redundancy can significantly reduce the number of colors needed, since recovering \\(H_{ij}\\) from a compressed column simultaneously gives us \\(H_{ji}\\) for free. The coloring operates on an adjacency graph whose vertices are variables and whose edges connect pairs \\(i, j\\) with \\(H_{ij} \\neq 0\\). Diagonal entries are always recoverable, so only off-diagonal nonzeros create edges.</p> <p><code>asdex</code> uses star coloring (Gebremedhin et al., 2005) on this graph: a proper coloring with the additional constraint that every path on 4 vertices uses at least 3 colors. This constraint ensures that for each off-diagonal nonzero \\(H_{ij}\\), at least one of \\(i\\) or \\(j\\) has a unique color among the other's neighbors, making every entry unambiguously recoverable from the compressed product. Star coloring typically needs far fewer colors than treating the Hessian as a general Jacobian and applying row or column coloring.</p>"},{"location":"explanation/coloring/#the-greedy-algorithm","title":"The Greedy Algorithm","text":"<p><code>asdex</code> colors graphs using a greedy algorithm with LargestFirst vertex ordering. Vertices are sorted by decreasing degree (number of conflicts), and each vertex is assigned the smallest color not already used by any of its neighbors. Handling high-degree vertices first tends to produce fewer colors in practice, because the most constrained vertices are colored while the most options are still available.</p> <p>The greedy algorithm does not guarantee an optimal coloring, but it is fast \u2014 \\(O(|V| + |E|)\\) in the size of the conflict graph \u2014 and produces good results for the sparsity patterns typically encountered in scientific computing.</p>"},{"location":"explanation/coloring/#references","title":"References","text":"<ul> <li>Revisiting Sparse Matrix Coloring and Bicoloring, Montoison et al. (2025)</li> <li>What Color Is Your Jacobian? Graph Coloring for Computing Derivatives, Gebremedhin et al. (2005)</li> <li>New Acyclic and Star Coloring Algorithms with Application to Computing Hessians, Gebremedhin et al. (2007)</li> <li>Efficient Computation of Sparse Hessians Using Coloring and Automatic Differentiation, Gebremedhin et al. (2009)</li> <li>ColPack: Software for graph coloring and related problems in scientific computing, Gebremedhin et al. (2013)</li> </ul>"},{"location":"explanation/sparsity-detection/","title":"Sparsity Detection","text":"<p>Sparsity detection determines which entries of a Jacobian can be nonzero. Given \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), it computes a binary sparsity pattern \u2014 a superset of the true nonzero structure of the Jacobian \\(J_{ij} = \\partial f_i / \\partial x_j\\). This pattern is the input to graph coloring, which exploits the sparsity to reduce the number of AD passes.</p>"},{"location":"explanation/sparsity-detection/#global-vs-local-sparsity-patterns","title":"Global vs. Local Sparsity Patterns","text":"<p>Consider \\(f(x) = x_1 \\cdot x_2\\), whose Jacobian is \\([x_2,\\; x_1]\\).</p> <p>A local pattern is the sparsity at a specific input point \\(x\\), so it depends on the numerical values. At \\(x = (1, 0)\\), the local pattern is <code>[0, 1]</code>, but at \\(x = (0, 1)\\) it is <code>[1, 0]</code>.</p> <p>A global pattern is the union of local patterns over the entire input domain, making it input-independent. For the same example, the global pattern is always <code>[1, 1]</code> (no sparsity). Global patterns are supersets of local patterns \u2014 less sparse, but valid everywhere.</p> <p><code>asdex</code> exclusively computes global patterns, so the result depends only on the function's structure, not on any particular input.</p>"},{"location":"explanation/sparsity-detection/#why-conservative-patterns-are-required","title":"Why Conservative Patterns Are Required","text":"<p>A global sparsity pattern used for coloring must be either accurate or conservative. If the pattern misses a nonzero entry, the coloring may merge rows or columns that actually conflict, silently producing wrong results. Overestimating extra nonzeros is safe \u2014 it just uses more colors than strictly necessary. This is why <code>asdex</code> errs on the side of conservatism: correctness comes first.</p>"},{"location":"explanation/sparsity-detection/#how-asdex-detects-sparsity","title":"How asdex Detects Sparsity","text":"<p><code>asdex</code> traces the function into JAX's intermediate representation (jaxpr), then propagates index sets forward through the computation graph. Each input element \\(x_j\\) starts with the singleton set \\(\\{j\\}\\), and each primitive operation propagates these sets according to its mathematical structure:</p> <ul> <li>Elementwise ops (sin, exp, add): preserve per-element sets.</li> <li>Reductions (sum): union all input sets.</li> <li>Indexing (gather/scatter): route sets based on index structure.</li> </ul> <p>The output index sets directly encode the sparsity pattern: output \\(i\\) depends on input \\(j\\) iff \\(j \\in S_i\\). No derivatives are evaluated \u2014 the analysis is purely structural.</p>"},{"location":"explanation/sparsity-detection/#sources-of-conservatism","title":"Sources of Conservatism","text":"<p>Three mechanisms make global patterns conservative:</p> <ol> <li>Branching (<code>cond</code>, <code>select_n</code>):    the detector takes the union over all branches,    since it cannot know which branch will execute at runtime.    This is the primary difference from local detection.</li> <li>Multiplication:    \\(f(x) = x_1 \\cdot x_2\\) always reports both dependencies globally,    even though one factor might be zero at a particular input.</li> <li>Fallback handlers:    primitives without a precise handler conservatively assume    every output depends on every input.</li> </ol> <p>Tip</p> <p>More precise handlers can be added for fallback primitives to reduce conservatism and produce sparser patterns. Please open an issue if you encounter overly conservative patterns. </p>"},{"location":"explanation/sparsity-detection/#hessian-detection","title":"Hessian Detection","text":"<p>Hessian sparsity is detected by applying Jacobian detection to the gradient:</p> \\[ \\operatorname{hessian\\_sparsity}(f) = \\operatorname{jacobian\\_sparsity}(\\nabla f) \\] <p>This composes naturally with JAX's autodiff: <code>jax.grad</code> produces a jaxpr, which <code>asdex</code> analyzes the same way.</p>"},{"location":"how-to/brusselator/","title":"Example: Brusselator PDE","text":"<p>This example computes the sparse Jacobian of a semi-discretized 2D reaction-diffusion system using <code>asdex</code>.</p>"},{"location":"how-to/brusselator/#the-brusselator-system","title":"The Brusselator System","text":"<p>The Brusselator models autocatalytic reactions on a 2D domain \\([0, 1]^2\\) with periodic boundary conditions:</p> \\[ \\frac{\\partial u}{\\partial t} = 1 + u^2 v - 4.4\\,u + \\alpha \\nabla^2 u \\] \\[ \\frac{\\partial v}{\\partial t} = 3.4\\,u - u^2 v + \\alpha \\nabla^2 v \\] <p>with diffusion coefficient \\(\\alpha = 10\\). See the SciML Brusselator tutorials for the full formulation including a localized forcing term, which is omitted here since it is state-independent and does not affect the Jacobian sparsity.</p>"},{"location":"how-to/brusselator/#discretizing-the-rhs","title":"Discretizing the RHS","text":"<p>Semi-discretize in space using second-order finite differences on an \\(N \\times N\\) grid. The state vector concatenates both species \\(u\\) and \\(v\\), giving \\(2N^2\\) unknowns:</p> <pre><code>import jax.numpy as jnp\n\nN = 32\nalpha = 10.0\ndx = 1.0 / N\n\ndef brusselator_rhs(uv):\n    u = uv[:N*N].reshape(N, N)\n    v = uv[N*N:].reshape(N, N)\n\n    # 5-point Laplacian with periodic boundary conditions\n    def laplacian(w):\n        return (\n            jnp.roll(w, 1, axis=0) + jnp.roll(w, -1, axis=0)\n            + jnp.roll(w, 1, axis=1) + jnp.roll(w, -1, axis=1)\n            - 4 * w\n        ) / dx**2\n\n    du = 1.0 + u**2 * v - 4.4 * u + alpha * laplacian(u)\n    dv = 3.4 * u - u**2 * v + alpha * laplacian(v)\n\n    return jnp.concatenate([du.ravel(), dv.ravel()])\n</code></pre>"},{"location":"how-to/brusselator/#detecting-and-coloring-the-jacobian","title":"Detecting and Coloring the Jacobian","text":"<p>The Jacobian has shape \\(2048 \\times 2048\\), but only 6 nonzeros per row (5 from the Laplacian stencil plus 1 from reaction coupling). Detect the sparsity and color it in one call:</p> <pre><code>from asdex import jacobian_coloring\n\ncolored_pattern = jacobian_coloring(brusselator_rhs, input_shape=2 * N * N)\n</code></pre> <pre><code>ColoredPattern(2048\u00d72048, nnz=12288, sparsity=99.7%, JVP, 12 colors)\n  12 JVPs (instead of 2048 VJPs or 2048 JVPs)\n\u23a1\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u28ff\u28ff\u28ff\u28ff\u28c3\u2840\u23a4\n\u23a2\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28cd\u2803\u23a5\n\u23a2\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28df\u28b7\u2801\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ef\u28db\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u286b\u2815\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ee\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2804\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28df\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28fd\u2800\u23a5\n\u23a2\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ee\u28c1\u23a5\n\u23a2\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u23a5 \u2192 \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u284f\u23a5\n\u23a2\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28df\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2803\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u28c4\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2877\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u28c4\u2800\u23a5   \u23a2\u28ff\u28ff\u28ff\u28ff\u28ff\u2844\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28bf\u28f7\u23a6   \u23a3\u28ff\u28ff\u28ff\u28ff\u28ff\u28f6\u23a6\n</code></pre> <p>Instead of 2048 JVPs or VJPs, <code>asdex</code> needs only as many as there are colors.</p>"},{"location":"how-to/brusselator/#computing-the-jacobian","title":"Computing the Jacobian","text":"<p>With the colored pattern precomputed, evaluate the sparse Jacobian at any state:</p> <pre><code>from asdex import jacobian\n\n# Brusselator initial condition\nx = jnp.linspace(0, 1, N, endpoint=False)\nxx, yy = jnp.meshgrid(x, x)\nu0 = 22.0 * (yy * (1 - yy)) ** 1.5\nv0 = 27.0 * (xx * (1 - xx)) ** 1.5\nuv0 = jnp.concatenate([u0.ravel(), v0.ravel()])\n\njac_fn = jacobian(brusselator_rhs, colored_pattern)\nJ = jac_fn(uv0)\n</code></pre> <pre><code>BCOO(float32[2048, 2048], nse=12288)\n</code></pre> <p>Make sure to reuse the <code>colored_pattern</code> across evaluations at different states, such that only the decompression step is repeated.</p>"},{"location":"how-to/brusselator/#references","title":"References","text":"<p>This example is based on tutorials from the SciML ecosystem. Consider giving the Julia programming language a shot, it is fantastic.</p> <ul> <li>NonlinearSolve.jl: Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia   (MIT License, Copyright (c) 2020 Julia Computing, Inc.)</li> <li>MethodOfLines.jl: Getting Started    (MIT License, Copyright (c) 2022 SciML Open Source Scientific Machine Learning Organization.)</li> </ul>"},{"location":"how-to/hessians/","title":"Computing Sparse Hessians","text":"<p><code>asdex</code> computes sparse Hessians for scalar-valued functions \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) using symmetric coloring and forward-over-reverse AD.</p>"},{"location":"how-to/hessians/#one-call-api","title":"One-Call API","text":"<p>The simplest way to compute a sparse Hessian:</p> <pre><code>from asdex import hessian\n\nH = hessian(f)(x)\n</code></pre> <p>This detects sparsity, colors the pattern symmetrically, and decompresses. The result is a JAX BCOO sparse matrix.</p> <p>Precompute the colored pattern</p> <p>Without a precomputed colored pattern, <code>hessian</code> re-detects sparsity and re-colors on every call. These steps are computationally expensive. If you call <code>hessian</code> more than once for the same function, precompute the colored pattern and reuse it \u2014 see below.</p>"},{"location":"how-to/hessians/#precomputing-the-colored-pattern","title":"Precomputing the Colored Pattern","text":"<p>When computing Hessians at many different inputs, precompute the colored pattern once:</p> <pre><code>from asdex import hessian_coloring, hessian\n\ncolored_pattern = hessian_coloring(g, input_shape=100)\nhess_fn = hessian(g, colored_pattern)\n\nfor x in inputs:\n    H = hess_fn(x)\n</code></pre> <p>The colored pattern depends only on the function structure, not the input values, so it can be reused across evaluations.</p>"},{"location":"how-to/hessians/#symmetric-coloring","title":"Symmetric Coloring","text":"<p>Hessians are symmetric (\\(H = H^\\top\\)), and <code>asdex</code> exploits this with star coloring (Gebremedhin et al., 2005). Symmetric coloring typically needs fewer colors than row or column coloring, since both \\(H_{ij}\\) and \\(H_{ji}\\) can be recovered from a single coloring.</p> <p>The convenience functions <code>hessian_coloring</code> and <code>hessian</code> use symmetric coloring automatically. Here we use the Rosenbrock function, a classic optimization benchmark whose Hessian is tridiagonal:</p> \\[f(x) = \\sum_{i=1}^{n-1} \\left[(1 - x_i)^2 + 100\\,(x_{i+1} - x_i^2)^2\\right]\\] <pre><code>import jax.numpy as jnp\nfrom asdex import hessian_coloring\n\ndef rosenbrock(x):\n    return jnp.sum((1 - x[:-1]) ** 2 + 100 * (x[1:] - x[:-1] ** 2) ** 2)\n\ncolored_pattern = hessian_coloring(rosenbrock, input_shape=100)\n</code></pre> <pre><code>ColoredPattern(100\u00d7100, nnz=298, sparsity=97.0%, HVP, 3 colors)\n  3 HVPs (instead of 100 HVPs)\n\u23a1\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u2877\u2847\u23a4\n\u23a2\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2845\u23a5\n\u23a2\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ef\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2845\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ef\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2845\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28df\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u2843\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2877\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u2800\u2800\u23a5   \u23a2\u28df\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u2840\u2800\u23a5   \u23a2\u28ff\u2806\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u28e6\u23a6   \u23a3\u28ff\u2803\u23a6\n</code></pre>"},{"location":"how-to/hessians/#separate-detection-and-coloring","title":"Separate Detection and Coloring","text":"<p>For more control, you can split detection and coloring:</p> <pre><code>from asdex import hessian_sparsity, color_hessian_pattern\n\nsparsity = hessian_sparsity(g, input_shape=100)\ncolored_pattern = color_hessian_pattern(sparsity)\n</code></pre> <p>This is useful when you want to inspect the sparsity pattern (<code>print(sparsity)</code>) before deciding on a coloring strategy.</p> <p>Since the Hessian is the Jacobian of the gradient, <code>hessian_sparsity</code> simply calls <code>jacobian_sparsity(jax.grad(f), input_shape)</code>. The sparsity interpreter composes naturally with JAX's autodiff transforms.</p>"},{"location":"how-to/hessians/#manually-providing-a-sparsity-pattern","title":"Manually Providing a Sparsity Pattern","text":"<p>You can provide a sparsity pattern manually if you already know it ahead of time. Create a <code>SparsityPattern</code> from coordinate arrays, a dense matrix, or a JAX BCOO matrix.</p> <p>From a dense boolean or numeric matrix:</p> <p><pre><code>import numpy as np\nfrom asdex import SparsityPattern\n\ndense = np.array([[1, 1, 0, 0],\n                  [1, 1, 1, 0],\n                  [0, 1, 1, 1],\n                  [0, 0, 1, 1]])\nsparsity = SparsityPattern.from_dense(dense)\n</code></pre> <pre><code>SparsityPattern(4\u00d74, nnz=10, sparsity=37.5%)\n\u25cf \u25cf \u22c5 \u22c5\n\u25cf \u25cf \u25cf \u22c5\n\u22c5 \u25cf \u25cf \u25cf\n\u22c5 \u22c5 \u25cf \u25cf\n</code></pre></p> <p>From row and column index arrays:</p> <p><pre><code>sparsity = SparsityPattern.from_coordinates(\n    rows=[0, 0, 1, 1, 1, 2, 2, 2, 3, 3],\n    cols=[0, 1, 0, 1, 2, 1, 2, 3, 2, 3],\n    shape=(4, 4),\n)\n</code></pre> <pre><code>SparsityPattern(4\u00d74, nnz=10, sparsity=37.5%)\n\u25cf \u25cf \u22c5 \u22c5\n\u25cf \u25cf \u25cf \u22c5\n\u22c5 \u25cf \u25cf \u25cf\n\u22c5 \u22c5 \u25cf \u25cf\n</code></pre></p> <p>From a JAX BCOO sparse matrix: <pre><code>sparsity = SparsityPattern.from_bcoo(bcoo_matrix)\n</code></pre></p> <p>Finally, color the sparsity pattern and compute the Hessian: <pre><code>from asdex import color_hessian_pattern, hessian\n\ncolored_pattern = color_hessian_pattern(sparsity)\nhess_fn = hessian(f, colored_pattern)\nH = hess_fn(x)\n</code></pre></p>"},{"location":"how-to/hessians/#multi-dimensional-inputs","title":"Multi-Dimensional Inputs","text":"<p><code>asdex</code> supports multi-dimensional input arrays. The Hessian is always returned as a 2D matrix of shape \\((n, n)\\) where \\(n\\) is the total number of input elements:</p> <pre><code>import jax.numpy as jnp\nfrom asdex import hessian_coloring\n\ndef g(x):\n    # x has shape (5, 20)\n    return jnp.sum(x ** 3)\n\ncolored_pattern = hessian_coloring(g, input_shape=(5, 20))\n</code></pre> <pre><code>ColoredPattern(100\u00d7100, nnz=100, sparsity=99.0%, HVP, 1 color)\n  1 HVP (instead of 100 HVPs)\n\u23a1\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u2847\u23a4\n\u23a2\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u23a5   \u23a2\u2847\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u23a6   \u23a3\u2847\u23a6\n</code></pre>"},{"location":"how-to/jacobians/","title":"Computing Sparse Jacobians","text":""},{"location":"how-to/jacobians/#one-call-api","title":"One-Call API","text":"<p>The simplest way to compute a sparse Jacobian:</p> <pre><code>from asdex import jacobian\n\nJ = jacobian(f)(x)\n</code></pre> <p>This detects sparsity, colors the pattern, and decompresses \u2014 all in one call. The result is a JAX BCOO sparse matrix.</p> <p>Precompute the colored pattern</p> <p>Without a precomputed colored pattern, <code>jacobian</code> re-detects sparsity and re-colors on every call. These steps are computationally expensive. If you call <code>jacobian</code> more than once for the same function, precompute the colored pattern and reuse it \u2014 see below.</p>"},{"location":"how-to/jacobians/#precomputing-the-colored-pattern","title":"Precomputing the Colored Pattern","text":"<p>When computing Jacobians at many different inputs, precompute the colored pattern once:</p> <pre><code>from asdex import jacobian_coloring, jacobian\n\ncolored_pattern = jacobian_coloring(f, input_shape=1000)\njac_fn = jacobian(f, colored_pattern)\n\nfor x in inputs:\n    J = jac_fn(x)\n</code></pre> <p>The colored pattern depends only on the function structure, not the input values, so it can be reused across evaluations.</p>"},{"location":"how-to/jacobians/#choosing-row-vs-column-coloring","title":"Choosing Row vs Column Coloring","text":"<p>By default, <code>asdex</code> tries both row and column coloring and picks whichever needs fewer colors:</p> <pre><code>from asdex import jacobian_coloring\n\ndef f(x):\n    return (x[1:] - x[:-1]) ** 2\n\n# Automatic selection (default):\ncolored_pattern = jacobian_coloring(f, input_shape=100)\n</code></pre> <pre><code>ColoredPattern(99\u00d7100, nnz=198, sparsity=98.0%, JVP, 2 colors)\n  2 JVPs (instead of 99 VJPs or 100 JVPs)\n\u23a1\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u28ff\u23a4\n\u23a2\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u23a6   \u23a3\u28ff\u23a6\n</code></pre> <p>You can also force a specific coloring strategy. Row coloring uses VJPs (reverse-mode AD), column coloring uses JVPs (forward-mode AD):</p> <pre><code># Force column coloring (uses JVPs):\ncolored_pattern = jacobian_coloring(f, input_shape=100, partition=\"column\")\n\n# Force row coloring (uses VJPs):\ncolored_pattern = jacobian_coloring(f, input_shape=100, partition=\"row\")\n</code></pre> <pre><code>ColoredPattern(99\u00d7100, nnz=198, sparsity=98.0%, VJP, 2 colors)\n  2 VJPs (instead of 99 VJPs or 100 JVPs)\n\u23a1\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4\n\u23a2\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28c6\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2839\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u2800\u2800\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u2800\u2800\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28c4\u23a6\n                     \u2193\n\u23a1\u281a\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u281b\u2813\u23a4\n</code></pre> <p>When the number of colors is equal, <code>asdex</code> prefers column coloring since JVPs are generally cheaper in JAX.</p>"},{"location":"how-to/jacobians/#separate-detection-and-coloring","title":"Separate Detection and Coloring","text":"<p>For more control, you can split detection and coloring:</p> <pre><code>from asdex import jacobian_sparsity, color_jacobian_pattern\n\nsparsity = jacobian_sparsity(f, input_shape=1000)\ncolored_pattern = color_jacobian_pattern(sparsity, partition=\"column\")\n</code></pre> <p>This is useful when you want to inspect the sparsity pattern (<code>print(sparsity)</code>) before deciding on a coloring strategy.</p>"},{"location":"how-to/jacobians/#manually-providing-a-sparsity-pattern","title":"Manually Providing a Sparsity Pattern","text":"<p>You can provide a sparsity pattern manually if you already know it ahead of time. Create a <code>SparsityPattern</code> from coordinate arrays, a dense matrix, or a JAX BCOO matrix.</p> <p>From a dense boolean or numeric matrix:</p> <p><pre><code>import numpy as np\nfrom asdex import SparsityPattern\n\ndense = np.array([[1, 1, 0, 0],\n                  [0, 1, 1, 0],\n                  [0, 0, 1, 1]])\nsparsity = SparsityPattern.from_dense(dense)\n</code></pre> <pre><code>SparsityPattern(3\u00d74, nnz=6, sparsity=50.0%)\n\u25cf \u25cf \u22c5 \u22c5\n\u22c5 \u25cf \u25cf \u22c5\n\u22c5 \u22c5 \u25cf \u25cf\n</code></pre></p> <p>From row and column index arrays:</p> <p><pre><code>sparsity = SparsityPattern.from_coordinates(\n    rows=[0, 0, 1, 1, 2, 2],\n    cols=[0, 1, 1, 2, 2, 3],\n    shape=(3, 4),\n)\n</code></pre> <pre><code>SparsityPattern(3\u00d74, nnz=6, sparsity=50.0%)\n\u25cf \u25cf \u22c5 \u22c5\n\u22c5 \u25cf \u25cf \u22c5\n\u22c5 \u22c5 \u25cf \u25cf\n</code></pre></p> <p>From a JAX BCOO sparse matrix: <pre><code>sparsity = SparsityPattern.from_bcoo(bcoo_matrix)\n</code></pre></p> <p>Finally, color the sparsity pattern and compute the Jacobian: <pre><code>from asdex import color_jacobian_pattern, jacobian\n\ncolored_pattern = color_jacobian_pattern(sparsity)\njac_fn = jacobian(f, colored_pattern)\nJ = jac_fn(x)\n</code></pre></p>"},{"location":"how-to/jacobians/#multi-dimensional-inputs","title":"Multi-Dimensional Inputs","text":"<p><code>asdex</code> supports multi-dimensional input and output arrays. The Jacobian is always returned as a 2D matrix of shape \\((m, n)\\) where \\(n\\) is the total number of input elements and \\(m\\) is the total number of output elements:</p> <pre><code>from asdex import jacobian_coloring\n\ndef f(x):\n    # x has shape (10, 10), output has shape (9, 10)\n    return x[1:, :] - x[:-1, :]\n\ncolored_pattern = jacobian_coloring(f, input_shape=(10, 10))\n</code></pre> <pre><code>ColoredPattern(90\u00d7100, nnz=180, sparsity=98.0%, JVP, 2 colors)\n  2 JVPs (instead of 90 VJPs or 100 JVPs)\n\u23a1\u2811\u2844\u2800\u2800\u2811\u2844\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u28ff\u23a4\n\u23a2\u2800\u2808\u2811\u2844\u2800\u2808\u2811\u2844\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2808\u2822\u2840\u2800\u2808\u2822\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2809\u2822\u2840\u2800\u2809\u2822\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2823\u2840\u2800\u2800\u2823\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2823\u2840\u2800\u2808\u2823\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2886\u2800\u2800\u2808\u2886\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2809\u2886\u2800\u2800\u2819\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u28c4\u2800\u2800\u2831\u28c0\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2831\u2840\u2800\u2800\u2831\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u28a2\u2840\u2800\u2808\u28a2\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u28a2\u2800\u2800\u2808\u28a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2884\u2800\u2800\u2811\u2884\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2822\u28c0\u2800\u2808\u2822\u28c0\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2822\u2840\u2800\u2808\u2822\u2840\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2818\u2884\u2840\u2800\u2818\u2884\u2840\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2818\u2884\u2800\u2800\u2818\u2884\u23a6   \u23a3\u28ff\u23a6\n</code></pre>"},{"location":"reference/","title":"Full API","text":""},{"location":"reference/#asdex.jacobian","title":"<code>asdex.jacobian(f, colored_pattern=None)</code>","text":"<p>Build a sparse Jacobian function using coloring and AD.</p> <p>Uses row coloring + VJPs or column coloring + JVPs, depending on which needs fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Function taking an array and returning an array. Input and output may be multi-dimensional.</p> required <code>colored_pattern</code> <code>ColoredPattern | None</code> <p>Optional pre-computed <code>ColoredPattern</code> from <code>jacobian_coloring</code>. If None, sparsity is detected and colored automatically on each call.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Jacobian as BCOO of shape <code>(m, n)</code> where <code>n = x.size</code> and <code>m = prod(output_shape)</code>.</p>"},{"location":"reference/#asdex.hessian","title":"<code>asdex.hessian(f, colored_pattern=None)</code>","text":"<p>Build a sparse Hessian function using coloring and HVPs.</p> <p>Uses symmetric (star) coloring and forward-over-reverse Hessian-vector products for efficiency.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Scalar-valued function taking an array. Input may be multi-dimensional.</p> required <code>colored_pattern</code> <code>ColoredPattern | None</code> <p>Optional pre-computed <code>ColoredPattern</code> from <code>hessian_coloring</code>. If None, sparsity is detected and colored automatically on each call.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Hessian as BCOO of shape <code>(n, n)</code> where <code>n = x.size</code>.</p>"},{"location":"reference/#asdex.jacobian_coloring","title":"<code>asdex.jacobian_coloring(f, input_shape, partition='auto')</code>","text":"<p>Detect Jacobian sparsity and color in one step.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>partition</code> <code>Literal['row', 'column', 'auto']</code> <p>Which partition to color (<code>\"row\"</code>, <code>\"column\"</code>, or <code>\"auto\"</code>).</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian</code>.</p>"},{"location":"reference/#asdex.hessian_coloring","title":"<code>asdex.hessian_coloring(f, input_shape)</code>","text":"<p>Detect Hessian sparsity and color in one step.</p> <p>Uses symmetric coloring, which exploits Hessian symmetry for fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input array.</p> required <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian</code>.</p>"},{"location":"reference/#asdex.jacobian_sparsity","title":"<code>asdex.jacobian_sparsity(f, input_shape)</code>","text":"<p>Detect global Jacobian sparsity pattern for f: R^n -&gt; R^m.</p> <p>Analyzes the computation graph structure directly, without evaluating any derivatives. The result is valid for all inputs.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(m, n)</code> where <code>n = prod(input_shape)</code> and <code>m = prod(output_shape)</code>. Entry <code>(i, j)</code> is present if output <code>i</code> depends on input <code>j</code>.</p>"},{"location":"reference/#asdex.hessian_sparsity","title":"<code>asdex.hessian_sparsity(f, input_shape)</code>","text":"<p>Detect global Hessian sparsity pattern for f: R^n -&gt; R.</p> <p>Analyzes the Jacobian sparsity of the gradient function, without evaluating any derivatives. The result is valid for all inputs.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(n, n)</code> where <code>n = prod(input_shape)</code>. Entry <code>(i, j)</code> is present if <code>H[i, j]</code> may be nonzero.</p>"},{"location":"reference/#asdex.color_jacobian_pattern","title":"<code>asdex.color_jacobian_pattern(sparsity, partition='auto')</code>","text":"<p>Color a sparsity pattern for sparse Jacobian computation.</p> <p>Assigns colors so that same-colored rows (or columns) can be computed together in a single VJP (or JVP).</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Sparsity pattern of shape (m, n).</p> required <code>partition</code> <code>Literal['row', 'column', 'auto']</code> <p>Which partition to color. <code>\"row\"</code> colors rows (uses VJPs), <code>\"column\"</code> colors columns (uses JVPs), <code>\"auto\"</code> picks whichever needs fewer colors (ties go to column coloring since JVPs are cheaper).</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian</code>.</p>"},{"location":"reference/#asdex.color_hessian_pattern","title":"<code>asdex.color_hessian_pattern(sparsity)</code>","text":"<p>Color a sparsity pattern for sparse Hessian computation.</p> <p>Uses symmetric coloring, which exploits Hessian symmetry for fewer colors than row coloring.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Symmetric sparsity pattern of shape (n, n).</p> required <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian</code>.</p>"},{"location":"reference/#asdex.color_rows","title":"<code>asdex.color_rows(sparsity)</code>","text":"<p>Greedy row-wise coloring for sparse Jacobian computation.</p> <p>Assigns colors to rows such that no two rows sharing a non-zero column have the same color. This enables computing multiple Jacobian rows in a single VJP by using a combined seed vector.</p> <p>Uses LargestFirst vertex ordering for fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>SparsityPattern of shape (m, n) representing the Jacobian sparsity pattern</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[int32], int]</code> <p>Tuple of (colors, num_colors) where:</p> <ul> <li>colors: Array of shape (m,) with color assignment for each row</li> <li>num_colors: Total number of colors used</li> </ul>"},{"location":"reference/#asdex.color_cols","title":"<code>asdex.color_cols(sparsity)</code>","text":"<p>Greedy column-wise coloring for sparse Jacobian computation.</p> <p>Assigns colors to columns such that no two columns sharing a non-zero row have the same color. This enables computing multiple Jacobian columns in a single JVP by using a combined tangent vector.</p> <p>Uses LargestFirst vertex ordering for fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>SparsityPattern of shape (m, n) representing the Jacobian sparsity pattern</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[int32], int]</code> <p>Tuple of (colors, num_colors) where:</p> <ul> <li>colors: Array of shape (n,) with color assignment for each column</li> <li>num_colors: Total number of colors used</li> </ul>"},{"location":"reference/#asdex.color_symmetric","title":"<code>asdex.color_symmetric(sparsity)</code>","text":"<p>Greedy symmetric coloring for sparse Hessian computation.</p> <p>Uses star coloring (Gebremedhin et al., 2005): a distance-1 coloring with the additional constraint that every path on 4 vertices uses at least 3 colors. This enables symmetric decompression using fewer colors than row coloring.</p> <p>Requires a square sparsity pattern (Hessians are always square). Uses LargestFirst vertex ordering.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>SparsityPattern of shape (n, n) representing the symmetric Hessian sparsity pattern</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[int32], int]</code> <p>Tuple of (colors, num_colors) where:</p> <ul> <li>colors: Array of shape (n,) with color assignment for each row/column</li> <li>num_colors: Total number of colors used</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pattern is not square</p>"},{"location":"reference/#asdex.SparsityPattern","title":"<code>asdex.SparsityPattern</code>  <code>dataclass</code>","text":"<p>Sparse matrix pattern storing only structural information (no values).</p> <p>Stores row and column indices separately for efficient access by the coloring and decompression stages.</p> <p>Attributes:</p> Name Type Description <code>rows</code> <code>NDArray[int32]</code> <p>Row indices of non-zero entries, shape <code>(nnz,)</code></p> <code>cols</code> <code>NDArray[int32]</code> <p>Column indices of non-zero entries, shape <code>(nnz,)</code></p> <code>shape</code> <code>tuple[int, int]</code> <p>Matrix dimensions <code>(m, n)</code></p> <code>input_shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the function input that produced this pattern. Defaults to <code>(n,)</code> if not specified.</p>"},{"location":"reference/#asdex.SparsityPattern.nnz","title":"<code>nnz</code>  <code>property</code>","text":"<p>Number of non-zero elements.</p>"},{"location":"reference/#asdex.SparsityPattern.m","title":"<code>m</code>  <code>property</code>","text":"<p>Number of rows.</p>"},{"location":"reference/#asdex.SparsityPattern.n","title":"<code>n</code>  <code>property</code>","text":"<p>Number of columns.</p>"},{"location":"reference/#asdex.SparsityPattern.density","title":"<code>density</code>  <code>property</code>","text":"<p>Fraction of non-zero entries.</p>"},{"location":"reference/#asdex.SparsityPattern.col_to_rows","title":"<code>col_to_rows</code>  <code>cached</code> <code>property</code>","text":"<p>Mapping from column index to list of row indices with non-zeros.</p> <p>Used by the coloring algorithm to build the row conflict graph.</p>"},{"location":"reference/#asdex.SparsityPattern.row_to_cols","title":"<code>row_to_cols</code>  <code>cached</code> <code>property</code>","text":"<p>Mapping from row index to list of column indices with non-zeros.</p> <p>Used by the coloring algorithm to build the column conflict graph.</p>"},{"location":"reference/#asdex.SparsityPattern.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set defaults.</p>"},{"location":"reference/#asdex.SparsityPattern.from_coordinates","title":"<code>from_coordinates(rows, cols, shape, *, input_shape=None)</code>  <code>classmethod</code>","text":"<p>Create pattern from row and column index arrays.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>NDArray[int32] | list[int]</code> <p>Row indices of non-zero entries.</p> required <code>cols</code> <code>NDArray[int32] | list[int]</code> <p>Column indices of non-zero entries.</p> required <code>shape</code> <code>tuple[int, int]</code> <p>Matrix dimensions <code>(m, n)</code>.</p> required <code>input_shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the function input. Defaults to <code>(n,)</code> if not specified.</p> <code>None</code>"},{"location":"reference/#asdex.SparsityPattern.from_bcoo","title":"<code>from_bcoo(bcoo)</code>  <code>classmethod</code>","text":"<p>Create pattern from JAX BCOO sparse matrix.</p>"},{"location":"reference/#asdex.SparsityPattern.from_dense","title":"<code>from_dense(dense)</code>  <code>classmethod</code>","text":"<p>Create pattern from dense boolean/numeric matrix.</p> <p>Non-zero entries indicate pattern positions.</p>"},{"location":"reference/#asdex.SparsityPattern.to_bcoo","title":"<code>to_bcoo(data=None)</code>","text":"<p>Convert to JAX BCOO sparse matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray | None</code> <p>Optional data values. If None, uses all 1s.</p> <code>None</code>"},{"location":"reference/#asdex.SparsityPattern.todense","title":"<code>todense()</code>","text":"<p>Convert to dense numpy array with 1s at pattern positions.</p>"},{"location":"reference/#asdex.SparsityPattern.__str__","title":"<code>__str__()</code>","text":"<p>Render sparsity pattern with header and dot/braille grid.</p>"},{"location":"reference/#asdex.SparsityPattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Return compact single-line representation.</p>"},{"location":"reference/#asdex.ColoredPattern","title":"<code>asdex.ColoredPattern</code>  <code>dataclass</code>","text":"<p>Result of a graph coloring for sparse differentiation.</p> <p>Attributes:</p> Name Type Description <code>sparsity</code> <code>SparsityPattern</code> <p>The sparsity pattern that was colored.</p> <code>colors</code> <code>NDArray[int32]</code> <p>Color assignment array. Shape <code>(m,)</code> for <code>\"VJP\"</code> mode, <code>(n,)</code> for <code>\"JVP\"</code> and <code>\"HVP\"</code> modes.</p> <code>num_colors</code> <code>int</code> <p>Total number of colors used.</p> <code>mode</code> <code>Literal['JVP', 'VJP', 'HVP']</code> <p>The AD primitive used per color. <code>\"VJP\"</code> for row-colored Jacobians, <code>\"JVP\"</code> for column-colored Jacobians, <code>\"HVP\"</code> for symmetrically colored Hessians.</p>"},{"location":"reference/#asdex.ColoredPattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Return compact single-line representation.</p>"},{"location":"reference/#asdex.ColoredPattern.__str__","title":"<code>__str__()</code>","text":"<p>Render colored pattern with sparsity grid and color assignments.</p>"},{"location":"reference/coloring/","title":"Coloring","text":""},{"location":"reference/coloring/#asdex.color_jacobian_pattern","title":"<code>asdex.color_jacobian_pattern(sparsity, partition='auto')</code>","text":"<p>Color a sparsity pattern for sparse Jacobian computation.</p> <p>Assigns colors so that same-colored rows (or columns) can be computed together in a single VJP (or JVP).</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Sparsity pattern of shape (m, n).</p> required <code>partition</code> <code>Literal['row', 'column', 'auto']</code> <p>Which partition to color. <code>\"row\"</code> colors rows (uses VJPs), <code>\"column\"</code> colors columns (uses JVPs), <code>\"auto\"</code> picks whichever needs fewer colors (ties go to column coloring since JVPs are cheaper).</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian</code>.</p>"},{"location":"reference/coloring/#asdex.color_hessian_pattern","title":"<code>asdex.color_hessian_pattern(sparsity)</code>","text":"<p>Color a sparsity pattern for sparse Hessian computation.</p> <p>Uses symmetric coloring, which exploits Hessian symmetry for fewer colors than row coloring.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Symmetric sparsity pattern of shape (n, n).</p> required <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian</code>.</p>"},{"location":"reference/coloring/#asdex.color_rows","title":"<code>asdex.color_rows(sparsity)</code>","text":"<p>Greedy row-wise coloring for sparse Jacobian computation.</p> <p>Assigns colors to rows such that no two rows sharing a non-zero column have the same color. This enables computing multiple Jacobian rows in a single VJP by using a combined seed vector.</p> <p>Uses LargestFirst vertex ordering for fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>SparsityPattern of shape (m, n) representing the Jacobian sparsity pattern</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[int32], int]</code> <p>Tuple of (colors, num_colors) where:</p> <ul> <li>colors: Array of shape (m,) with color assignment for each row</li> <li>num_colors: Total number of colors used</li> </ul>"},{"location":"reference/coloring/#asdex.color_cols","title":"<code>asdex.color_cols(sparsity)</code>","text":"<p>Greedy column-wise coloring for sparse Jacobian computation.</p> <p>Assigns colors to columns such that no two columns sharing a non-zero row have the same color. This enables computing multiple Jacobian columns in a single JVP by using a combined tangent vector.</p> <p>Uses LargestFirst vertex ordering for fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>SparsityPattern of shape (m, n) representing the Jacobian sparsity pattern</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[int32], int]</code> <p>Tuple of (colors, num_colors) where:</p> <ul> <li>colors: Array of shape (n,) with color assignment for each column</li> <li>num_colors: Total number of colors used</li> </ul>"},{"location":"reference/coloring/#asdex.color_symmetric","title":"<code>asdex.color_symmetric(sparsity)</code>","text":"<p>Greedy symmetric coloring for sparse Hessian computation.</p> <p>Uses star coloring (Gebremedhin et al., 2005): a distance-1 coloring with the additional constraint that every path on 4 vertices uses at least 3 colors. This enables symmetric decompression using fewer colors than row coloring.</p> <p>Requires a square sparsity pattern (Hessians are always square). Uses LargestFirst vertex ordering.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>SparsityPattern of shape (n, n) representing the symmetric Hessian sparsity pattern</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[int32], int]</code> <p>Tuple of (colors, num_colors) where:</p> <ul> <li>colors: Array of shape (n,) with color assignment for each row/column</li> <li>num_colors: Total number of colors used</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pattern is not square</p>"},{"location":"reference/data-structures/","title":"Data Structures","text":""},{"location":"reference/data-structures/#asdex.ColoredPattern","title":"<code>asdex.ColoredPattern</code>  <code>dataclass</code>","text":"<p>Result of a graph coloring for sparse differentiation.</p> <p>Attributes:</p> Name Type Description <code>sparsity</code> <code>SparsityPattern</code> <p>The sparsity pattern that was colored.</p> <code>colors</code> <code>NDArray[int32]</code> <p>Color assignment array. Shape <code>(m,)</code> for <code>\"VJP\"</code> mode, <code>(n,)</code> for <code>\"JVP\"</code> and <code>\"HVP\"</code> modes.</p> <code>num_colors</code> <code>int</code> <p>Total number of colors used.</p> <code>mode</code> <code>Literal['JVP', 'VJP', 'HVP']</code> <p>The AD primitive used per color. <code>\"VJP\"</code> for row-colored Jacobians, <code>\"JVP\"</code> for column-colored Jacobians, <code>\"HVP\"</code> for symmetrically colored Hessians.</p>"},{"location":"reference/data-structures/#asdex.ColoredPattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Return compact single-line representation.</p>"},{"location":"reference/data-structures/#asdex.ColoredPattern.__str__","title":"<code>__str__()</code>","text":"<p>Render colored pattern with sparsity grid and color assignments.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern","title":"<code>asdex.SparsityPattern</code>  <code>dataclass</code>","text":"<p>Sparse matrix pattern storing only structural information (no values).</p> <p>Stores row and column indices separately for efficient access by the coloring and decompression stages.</p> <p>Attributes:</p> Name Type Description <code>rows</code> <code>NDArray[int32]</code> <p>Row indices of non-zero entries, shape <code>(nnz,)</code></p> <code>cols</code> <code>NDArray[int32]</code> <p>Column indices of non-zero entries, shape <code>(nnz,)</code></p> <code>shape</code> <code>tuple[int, int]</code> <p>Matrix dimensions <code>(m, n)</code></p> <code>input_shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the function input that produced this pattern. Defaults to <code>(n,)</code> if not specified.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.nnz","title":"<code>nnz</code>  <code>property</code>","text":"<p>Number of non-zero elements.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.m","title":"<code>m</code>  <code>property</code>","text":"<p>Number of rows.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.n","title":"<code>n</code>  <code>property</code>","text":"<p>Number of columns.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.density","title":"<code>density</code>  <code>property</code>","text":"<p>Fraction of non-zero entries.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.col_to_rows","title":"<code>col_to_rows</code>  <code>cached</code> <code>property</code>","text":"<p>Mapping from column index to list of row indices with non-zeros.</p> <p>Used by the coloring algorithm to build the row conflict graph.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.row_to_cols","title":"<code>row_to_cols</code>  <code>cached</code> <code>property</code>","text":"<p>Mapping from row index to list of column indices with non-zeros.</p> <p>Used by the coloring algorithm to build the column conflict graph.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set defaults.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.from_coordinates","title":"<code>from_coordinates(rows, cols, shape, *, input_shape=None)</code>  <code>classmethod</code>","text":"<p>Create pattern from row and column index arrays.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>NDArray[int32] | list[int]</code> <p>Row indices of non-zero entries.</p> required <code>cols</code> <code>NDArray[int32] | list[int]</code> <p>Column indices of non-zero entries.</p> required <code>shape</code> <code>tuple[int, int]</code> <p>Matrix dimensions <code>(m, n)</code>.</p> required <code>input_shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the function input. Defaults to <code>(n,)</code> if not specified.</p> <code>None</code>"},{"location":"reference/data-structures/#asdex.SparsityPattern.from_bcoo","title":"<code>from_bcoo(bcoo)</code>  <code>classmethod</code>","text":"<p>Create pattern from JAX BCOO sparse matrix.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.from_dense","title":"<code>from_dense(dense)</code>  <code>classmethod</code>","text":"<p>Create pattern from dense boolean/numeric matrix.</p> <p>Non-zero entries indicate pattern positions.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.to_bcoo","title":"<code>to_bcoo(data=None)</code>","text":"<p>Convert to JAX BCOO sparse matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray | None</code> <p>Optional data values. If None, uses all 1s.</p> <code>None</code>"},{"location":"reference/data-structures/#asdex.SparsityPattern.todense","title":"<code>todense()</code>","text":"<p>Convert to dense numpy array with 1s at pattern positions.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.__str__","title":"<code>__str__()</code>","text":"<p>Render sparsity pattern with header and dot/braille grid.</p>"},{"location":"reference/data-structures/#asdex.SparsityPattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Return compact single-line representation.</p>"},{"location":"reference/hessian/","title":"Hessian","text":""},{"location":"reference/hessian/#asdex.hessian","title":"<code>asdex.hessian(f, colored_pattern=None)</code>","text":"<p>Build a sparse Hessian function using coloring and HVPs.</p> <p>Uses symmetric (star) coloring and forward-over-reverse Hessian-vector products for efficiency.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Scalar-valued function taking an array. Input may be multi-dimensional.</p> required <code>colored_pattern</code> <code>ColoredPattern | None</code> <p>Optional pre-computed <code>ColoredPattern</code> from <code>hessian_coloring</code>. If None, sparsity is detected and colored automatically on each call.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Hessian as BCOO of shape <code>(n, n)</code> where <code>n = x.size</code>.</p>"},{"location":"reference/hessian/#asdex.hessian_coloring","title":"<code>asdex.hessian_coloring(f, input_shape)</code>","text":"<p>Detect Hessian sparsity and color in one step.</p> <p>Uses symmetric coloring, which exploits Hessian symmetry for fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input array.</p> required <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian</code>.</p>"},{"location":"reference/hessian/#asdex.hessian_sparsity","title":"<code>asdex.hessian_sparsity(f, input_shape)</code>","text":"<p>Detect global Hessian sparsity pattern for f: R^n -&gt; R.</p> <p>Analyzes the Jacobian sparsity of the gradient function, without evaluating any derivatives. The result is valid for all inputs.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(n, n)</code> where <code>n = prod(input_shape)</code>. Entry <code>(i, j)</code> is present if <code>H[i, j]</code> may be nonzero.</p>"},{"location":"reference/hessian/#asdex.color_hessian_pattern","title":"<code>asdex.color_hessian_pattern(sparsity)</code>","text":"<p>Color a sparsity pattern for sparse Hessian computation.</p> <p>Uses symmetric coloring, which exploits Hessian symmetry for fewer colors than row coloring.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Symmetric sparsity pattern of shape (n, n).</p> required <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>hessian</code>.</p>"},{"location":"reference/jacobian/","title":"Jacobian","text":""},{"location":"reference/jacobian/#asdex.jacobian","title":"<code>asdex.jacobian(f, colored_pattern=None)</code>","text":"<p>Build a sparse Jacobian function using coloring and AD.</p> <p>Uses row coloring + VJPs or column coloring + JVPs, depending on which needs fewer colors.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[ArrayLike], ArrayLike]</code> <p>Function taking an array and returning an array. Input and output may be multi-dimensional.</p> required <code>colored_pattern</code> <code>ColoredPattern | None</code> <p>Optional pre-computed <code>ColoredPattern</code> from <code>jacobian_coloring</code>. If None, sparsity is detected and colored automatically on each call.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[ArrayLike], BCOO]</code> <p>A function that takes an input array and returns the sparse Jacobian as BCOO of shape <code>(m, n)</code> where <code>n = x.size</code> and <code>m = prod(output_shape)</code>.</p>"},{"location":"reference/jacobian/#asdex.jacobian_coloring","title":"<code>asdex.jacobian_coloring(f, input_shape, partition='auto')</code>","text":"<p>Detect Jacobian sparsity and color in one step.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>partition</code> <code>Literal['row', 'column', 'auto']</code> <p>Which partition to color (<code>\"row\"</code>, <code>\"column\"</code>, or <code>\"auto\"</code>).</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian</code>.</p>"},{"location":"reference/jacobian/#asdex.jacobian_sparsity","title":"<code>asdex.jacobian_sparsity(f, input_shape)</code>","text":"<p>Detect global Jacobian sparsity pattern for f: R^n -&gt; R^m.</p> <p>Analyzes the computation graph structure directly, without evaluating any derivatives. The result is valid for all inputs.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(m, n)</code> where <code>n = prod(input_shape)</code> and <code>m = prod(output_shape)</code>. Entry <code>(i, j)</code> is present if output <code>i</code> depends on input <code>j</code>.</p>"},{"location":"reference/jacobian/#asdex.color_jacobian_pattern","title":"<code>asdex.color_jacobian_pattern(sparsity, partition='auto')</code>","text":"<p>Color a sparsity pattern for sparse Jacobian computation.</p> <p>Assigns colors so that same-colored rows (or columns) can be computed together in a single VJP (or JVP).</p> <p>Parameters:</p> Name Type Description Default <code>sparsity</code> <code>SparsityPattern</code> <p>Sparsity pattern of shape (m, n).</p> required <code>partition</code> <code>Literal['row', 'column', 'auto']</code> <p>Which partition to color. <code>\"row\"</code> colors rows (uses VJPs), <code>\"column\"</code> colors columns (uses JVPs), <code>\"auto\"</code> picks whichever needs fewer colors (ties go to column coloring since JVPs are cheaper).</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>ColoredPattern</code> <p>A <code>ColoredPattern</code> ready for <code>jacobian</code>.</p>"},{"location":"reference/sparsity/","title":"Sparsity Detection","text":""},{"location":"reference/sparsity/#asdex.jacobian_sparsity","title":"<code>asdex.jacobian_sparsity(f, input_shape)</code>","text":"<p>Detect global Jacobian sparsity pattern for f: R^n -&gt; R^m.</p> <p>Analyzes the computation graph structure directly, without evaluating any derivatives. The result is valid for all inputs.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Function taking an array and returning an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(m, n)</code> where <code>n = prod(input_shape)</code> and <code>m = prod(output_shape)</code>. Entry <code>(i, j)</code> is present if output <code>i</code> depends on input <code>j</code>.</p>"},{"location":"reference/sparsity/#asdex.hessian_sparsity","title":"<code>asdex.hessian_sparsity(f, input_shape)</code>","text":"<p>Detect global Hessian sparsity pattern for f: R^n -&gt; R.</p> <p>Analyzes the Jacobian sparsity of the gradient function, without evaluating any derivatives. The result is valid for all inputs.</p> <p>If <code>f</code> returns a squeezable shape like <code>(1,)</code> or <code>(1, 1)</code>, it is automatically squeezed to scalar.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>Scalar-valued function taking an array.</p> required <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Shape of the input array. An integer is treated as a 1D length.</p> required <p>Returns:</p> Type Description <code>SparsityPattern</code> <p>SparsityPattern of shape <code>(n, n)</code> where <code>n = prod(input_shape)</code>. Entry <code>(i, j)</code> is present if <code>H[i, j]</code> may be nonzero.</p>"},{"location":"tutorials/getting-started/","title":"Getting Started","text":"<p>This tutorial walks through the three stages of automatic sparse differentiation: detection, coloring, and decompression.</p>"},{"location":"tutorials/getting-started/#the-problem","title":"The Problem","text":"<p>For a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), computing the full Jacobian \\(J \\in \\mathbb{R}^{m \\times n}\\) requires \\(n\\) forward-mode or \\(m\\) reverse-mode AD passes. In practice, for example in scientific machine learning, many Jacobians are sparse (i.e., most entries are structurally zero, regardless of the input).</p> <p><code>asdex</code> exploits this sparsity in three steps: 1. Detect the sparsity pattern by tracing the computation graph 2. Color the pattern so that structurally orthogonal rows (or columns) share a color 3. Decompress one AD pass per color into the sparse Jacobian or Hessian</p> <p>This reduces the computational cost from \\(m\\) (or \\(n\\)) AD passes to just the number of colors, yielding significant speedups on large sparse problems, especially when the cost of detection and coloring can be amortized over repeated evaluations. The same approach applies to sparse Hessians via forward-over-reverse AD.</p>"},{"location":"tutorials/getting-started/#sparse-jacobians","title":"Sparse Jacobians","text":"<p>Consider the squared differences function \\(f(x)_i = (x_{i+1} - x_i)^2\\), which has a banded Jacobian. Detect the sparsity pattern and color it in one step:</p> <pre><code>from asdex import jacobian_coloring\n\ndef f(x):\n    return (x[1:] - x[:-1]) ** 2\n\ncolored_pattern = jacobian_coloring(f, input_shape=50)\n</code></pre> <pre><code>ColoredPattern(49\u00d750, nnz=98, sparsity=96.0%, JVP, 2 colors)\n  2 JVPs (instead of 49 VJPs or 50 JVPs)\n\u23a1\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a4   \u23a1\u28ff\u23a4\n\u23a2\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5 \u2192 \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u2800\u2800\u23a5   \u23a2\u28ff\u23a5\n\u23a2\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28a6\u2840\u23a5   \u23a2\u28ff\u23a5\n\u23a3\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2809\u23a6   \u23a3\u2809\u23a6\n</code></pre> <p>The print-out shows the original sparsity pattern (left) compressed into just two colors (right). <code>asdex</code> automatically ran multiple coloring algorithms, selected column coloring (2 JVPs) over row coloring (2 VJPs) since JVPs are cheaper, reducing the cost from 49 VJPs or 50 JVPs without coloring to just 2 JVPs. Note that on small problems, this doesn't directly translate into a speedup of factor 25x, as the decompression overhead dominates.</p> <p>Global Sparsity Patterns</p> <p>The detected pattern is a global sparsity pattern: it depends only on the function's structure, not on any particular input. This means it may contain extra nonzeros compared to the sparsity at a specific point, but it is guaranteed to be correct everywhere. If you encounter overly conservative patterns, please open an issue. See Sparsity Detection for details.</p> <p>Now we can compute the sparse Jacobian using the colored pattern:</p> <pre><code>import jax.numpy as jnp\nfrom asdex import jacobian\n\nx = jnp.ones(50)\njac_fn = jacobian(f, colored_pattern)\nJ = jac_fn(x)\n</code></pre> <p>The result is a JAX BCOO sparse matrix. We can verify that <code>asdex</code> produces the same result as <code>jax.jacobian</code>:</p> <pre><code>import jax\nimport numpy as np\n\nJ_dense = jax.jacobian(f)(x)\nnp.testing.assert_allclose(J.todense(), J_dense, atol=1e-6)\n</code></pre> <p>On larger problems, the speedup from coloring becomes significant. Let's benchmark on a 5000-dimensional input (note that timings may vary as part of the doc-building process):</p> <pre><code>import timeit\n\nn = 5000\ncolored_pattern = jacobian_coloring(f, input_shape=n)\nx = jnp.ones(n)\n\njac_fn_asdex = jacobian(f, colored_pattern)\njac_fn_jax = jax.jacobian(f)\n\n# Warm up JIT caches\n_ = jac_fn_asdex(x)\n_ = jac_fn_jax(x)\n\nt_asdex = timeit.timeit(lambda: jac_fn_asdex(x).block_until_ready(), number=10) / 10\nt_jax = timeit.timeit(lambda: jac_fn_jax(x).block_until_ready(), number=10) / 10\n</code></pre> <pre><code>asdex.jacobian:      5.46 ms\njax.jacobian:      598.59 ms\nspeedup:            109.7x\n</code></pre> <p>Precompute for Repeated Evaluations</p> <p>The colored pattern depends only on the function structure, not the input values. When computing Jacobians at many different inputs, precompute the colored pattern once and reuse it:</p> <pre><code>jac_fn = jacobian(f, jacobian_coloring(f, input_shape=5000))\n\nfor x in inputs:\n    J = jac_fn(x)\n</code></pre>"},{"location":"tutorials/getting-started/#sparse-hessians","title":"Sparse Hessians","text":"<p>For scalar-valued functions \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\), <code>asdex</code> can detect Hessian sparsity and compute sparse Hessians:</p> <pre><code>from asdex import hessian_coloring, hessian\n\ndef g(x):\n    return jnp.sum(x ** 2)\n\nhess_fn = hessian(g, hessian_coloring(g, input_shape=20))\n\nfor x in inputs:\n    H = hess_fn(x)\n</code></pre>"},{"location":"tutorials/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Computing Sparse Jacobians \u2014 Guide on Jacobian computation</li> <li>Computing Sparse Hessians \u2014 Guide on Hessian computation</li> <li>Sparsity Detection \u2014 Explanation how sparsity patterns are detected</li> <li>Graph Coloring \u2014 Explanation how coloring reduces cost</li> </ul>"}]}